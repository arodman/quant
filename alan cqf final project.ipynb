{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db531ae",
   "metadata": {},
   "source": [
    "# CQF EXAM Final Project\n",
    "\n",
    "# Blending Ensemble for Classification (ML)\n",
    "# Deep Learning for Financial Time Series (DL)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Summary Table\n",
    "\n",
    "| CQF Candidate   |\n",
    "| ----------- |\n",
    "| Alan Rodriguez |\n",
    "| June 2023 Cohort |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbe257",
   "metadata": {},
   "source": [
    "### Table Contents <a id=\"table_contents\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3a449",
   "metadata": {},
   "source": [
    "(1) [Introduction] (#introduction)\n",
    "- [Objective](#ensemble)\n",
    "- [Methodology: Ensemble Learning](#ensemble)\n",
    "- [Methodology: Hyperparameter Tuning](#ensemble)\n",
    "\n",
    "(2) Feature Engienering (#feature)\n",
    "- [Data decription]\n",
    "- [ Sentiment Data]\n",
    "- [ Dummy Variables](#dummy)\n",
    "- [Lags](#technical)\n",
    "- [Technical Indicators](#technical)\n",
    "- [ Momentum Indicator]\n",
    "- [  Logarithmic Return]\n",
    "- [ Exponential Moving Average (EMA)]\n",
    "- [  Relative Strenght Index (RSI)]\n",
    "- [  Stochastic Oscillator]\n",
    "- [  Moving Average Convergence Divergence (MACD)]\n",
    "- [  Commodity Channel Index (CCI)]\n",
    "- [  Average True Range (ATR)]\n",
    "- [  Bollinger Bands (BB)]\n",
    "\n",
    "\n",
    "(3) Data Pre-processing a#\n",
    "6.5.3  Self-Orgnizing Maps: Feature Selection\n",
    "6.5.4  The Aggregation of Datasets\n",
    "- [Feature Selection](#Q3step6)\n",
    "- [Visualization of feature importance](#Q3step7)\n",
    "- [Analysis of Variance & t-test](#Q3step8)\n",
    "- [Clean and Transform data](#Q3step9)\n",
    "- 3.1 Dropping null values\n",
    "- 3.3 Feature Selection\n",
    "- 3.4 Feature Encoding\n",
    "- 3.5 Train-Test split\n",
    "\n",
    "\n",
    "(4)  Exploratory Data Analysis EDA \n",
    "- [Understanding the data](#eda)  \n",
    " - [Data Exploration](#Q3step2)\n",
    " - [z. Data Description](#data_description)     \n",
    "- [z. Exploratory Data Analysis EDA](#eda)     \n",
    "- [Creating Target](#Q3step3)\n",
    "- [Feature Engineering and Penalized Classification](#Q3step4)\n",
    "- [Correlation Analysis](#Q3step5)\n",
    "\n",
    "\n",
    "(5) Model Building\n",
    "- [iNTRO Modelling and Hyperparameters Tuning](#Q3step10)\n",
    "- [z. Logistic regression](#logistic) \n",
    "- [Decision Tree Classifier](#dt) \n",
    "- [Decision Tree Classifier with HyperParameter Tuning](#dt_ht) \n",
    "- [Random Forest Classifier RF](#randomf) \n",
    "- [Random Forest Classifier Hyperparameter tuning : Random Forest Classifier](#randomf_ht) \n",
    "- [Extra Trees Classifier](#et) \n",
    "- [Extra Trees Classifier HyperParameter Tuning](#et_ht) \n",
    "- [Bagging Classifier](#bag) \n",
    "- [[Bagging Classifier with Hyperparameter tuning : Bagging Classifier](#bag_ht) \n",
    "- [Support Vector Machines Classifier ](#svc) \n",
    "- [LinearSVC - Lienar Support Vector Machines Classifier](#svcl) \n",
    "- [AdaBoostClassifier (#adaboost) ](#adaboost) \n",
    "- [GradientBoostingClassifier](#grad) \n",
    "- [XGBClassifier (XGBoost)](#xgb) \n",
    "- [CatBoostClassifier](#cat) \n",
    "- [LGBMClassifier (LightGBM)4.2.3 Light Gradient Boosting Machine Regression LGB learning rate](#lgbmc) \n",
    "- [ Voting Classifier](#voting) \n",
    "- [ Voting Classifier: hard_voting](#voting) \n",
    "- [ Evaluation: Backtesting and Trading Strategy](#Q3step14)  \n",
    "- [ Voting Classifier:soft_voting](#voting) \n",
    "- [ Evaluation: Backtesting and Trading Strategy](#Q3step14)  \n",
    "\n",
    "(6)\n",
    "- [z. Conclusion](#conclusion)     \n",
    "- [z. Appendix](#appendix)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34577c3",
   "metadata": {},
   "source": [
    "### Introduction <a id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94130043",
   "metadata": {},
   "source": [
    "**Man objective:**\n",
    "- The goal is to forecast the rally / sellfof ivnersion ins US Treasury Rates, through an ensemble Machine Learning model. \n",
    "- The data correpsonds ot he US tREASURY curve and it spans all availabope maturities publised by FRED\n",
    "- aS independent variable we will sue indogneoeus variabels with lags, economic data, policy data, amonth other ( expkained in part 1.3)\n",
    "- The time hrzinog ons the last 10 yearrs on a daily basis. for abinary classificaiotn forecasts model\n",
    "- The traget is a binomial to speicify the specifc episodes(Daily binomial classification) (explained in oart 2,3)\n",
    "Trend Predictopn for investment managemente s\n",
    "- By using a combination of finaicla, and economics data ,we will build amodel to forecasts nad then we will ackstest the stretegy,  \n",
    "\n",
    "- We will use an Ensmele learning models  Heterogoeuns \n",
    "That owuld using voting in tis two versioins: (i) Voting majorithard (ii) Sdoft sofgrts uniform adn weightedn \n",
    "Hard voting predcitn the final class lables as the class labes that has been predicted most frequnetly bu the cl;assificaiton models\n",
    "Soft voting poredict the class lable by averaging ht eclass probabiltilies\n",
    "Majority preduct the class lable  y visa a majority voitng or each classifiers\n",
    "Weigited majority\n",
    "Compute the weighted majority vote by associateing weights w with calassifiers C\n",
    "\n",
    "**Process:** \n",
    "    - First we start laoding the data, both the rates data the economic data adn the finaical markets data (spx, vix, etc). We create the seasoality dummies. After we create some features. Economic data is statioanrytrty proces (z scores) in a previous stepp . creaddting the lags, the tehcical indicors n the tearget variables.\n",
    "    -secondly, exploratary data anayliss we analyze the data, worthi mentioend, that\n",
    "    - Feature engieneering, Thirdly we do some feature engienering, scaalna standarize, Also we carry a random forest algorith to idnetigy whihc are the 100 varaible  most singificats, in order to optimize the priocessing time. \n",
    " -Before we can start witht he voting model we go devleope each of the compeonet sof the vioting model : \n",
    " (Lso regressionFreost ragesSVM linears. \n",
    "        - [z. Logistic regression](#logistic) \n",
    "        - 4.1 Decision Tree Classifier\n",
    "        - 4.2 HyperParameter Tuning : Decision Tree Classifier\n",
    "        - 4.5 Random Forest Classifier\n",
    "        - 4.6 Hyperparameter tuning : Random Forest Classifier\n",
    "        - 4.7 Extra Trees Classifier\n",
    "        - 4.8 HyperParameter Tuning : Extra Tree Classifier\n",
    "        - 4.3 Bagging Classifier\n",
    "        - 4.4 Hyperparameter tuning : Bagging Classifier\n",
    "        AdaBoostClassifier\n",
    "        GradientBoostingClassifier\n",
    "        Support Vector Machines Classifier \n",
    "        LinearSVC - Lienar Support Vector Machines Classifier\n",
    "        S\n",
    "        Other:\n",
    "        XGBClassifier (XGBoost)\n",
    "        CatBoostClassifier\n",
    "        LGBMClassifier (LightGBM)\n",
    "    - Fifhte , we deveop the Voting cliasifierns model in it s two versions adn we check the resutls independintlty. \n",
    "        In this case since we already the woth the hypermaparamters tunnignalready found the range where the model a ahs a bbetter firt, we make the hyperparamter tuning in a much narrow search ,  \n",
    "        Wwe use them adn developo and soft voting model, \n",
    "        Sixt, we then bakctest the resutls. \n",
    "        Finally, we fond a strateg et that is able to fortecasta 75% of time oif ewe are going to doenter a rally,. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15521b7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33a1fca0",
   "metadata": {},
   "source": [
    "\n",
    "### Ensemble Learning Voting Classifier\n",
    "\n",
    "A ***Voting Classifier*** (Supervised ML) is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\n",
    "\n",
    "First lets define **Ensemble Learning**:\n",
    "\n",
    "- **Ensemble Learning methods** are machine learning techniques that combines a concrete finite set of alternative models in order to produce one optimal predictive model with a better overall performance. Its inside the family of  Supervised Learning models.\n",
    "- ***Ensemble Learning enhances the accuracy and resilience of the forecast by merging the predictions from multiple models while also aiming to mitigate errors or biases that could exist in individual models by using the aggregate intelligence of the ensemble model***. This approach not only enhances accuracy but also provides resilience against uncertainties in the data.\n",
    "- Ensemble models combine multiple hypotheses to form a better hypothesis.\n",
    "\n",
    "\n",
    "Lets explain the ***different Ensemble techniques***: it can be classified as Heterogenous and Homogeneous models \n",
    "- ***Voting Classifiers (Heterogeneous)*** can be divided in Majority, Weighted or Soft Voting (*more details below*)\n",
    "    - ***Majority or Max Voting***: each model is considered a vote. The predictions which gets the majority of the model's votes are used as the final prediction. In other words, it predicts the class label that has been predicted most frequently by the classification models.\n",
    "    - ***Soft or Averaging Voting***: average the predictions from all models.\n",
    "    - ***Weighted Voting***:  Each models is assigned a specific weights, which defines the importance of each model for prediction outcome. \n",
    "- ***Stacking (Heterogenous)***: uses the predictions from multiple models (for example decision tree, knn or svm) to build a new model.Basically it involves fitting many different model types on the same data to learn the best combination for the predictions.\n",
    "- ***Blending***: uses only a holdout set (validation) from the train set to make predictions.\n",
    "- ***Bagging or Bootstrap Aggregating (Homogenous)***: this technique uses different subsets (bags) to get a fair idea of the distribution (complete set). Bagging involves fitting many decision trees on different sets  of the same dataset and averaging those predictions.\n",
    "- ***Boosting (Homogenous)***: involves adding ensemble models sequentially that correct the predictions made by prior models and outputs a weighted average of the predictions.\n",
    "\n",
    "\n",
    "**Voting Classifiers with Examples:** What is voting classifier?\n",
    "\n",
    "- A Voting Classifier aggregates the findings of each classifier and predicts the output class based on the pre set decision rule.\n",
    "- Instead of creating separate models and finding the accuracy for each model, it generates a single model which is trained by the subset of individual models and predicts the output class based on their combined results. \n",
    "\n",
    "***Hard or Majority Voting Classifier:***\n",
    "- In a hard voting classifier, each individual classifier model in the ensemble makes a single prediction, and the final prediction is determined by a majority vote. The class that receives the most votes among the individual classifiers is the predicted class.\n",
    "\n",
    "***Soft or Average Voting Classifier:***\n",
    "- In a soft voting classifier, the individual classifiers not only make class predictions but also provide probability estimates for each class. The final prediction is determined by averaging the class probabilities from each classifier and selecting the class with the highest average probability.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2b7fd",
   "metadata": {},
   "source": [
    "  \n",
    "## Hyperarameters Tuning  \n",
    " \n",
    " \n",
    " Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning\n",
    " Model parameters are learned during training when we optimize a loss function\n",
    " Some set of parameters that are used to control the behaviour of the model/algorithm and adjustable in order to obtain an improvised model with optimal performance is so-called Hyperparameters.\n",
    " \n",
    " Whereas the model parameters specify how to transform the input data into the desired output, the hyperparameters define how our model is actually structured\n",
    " \n",
    "  hyperparameter tuning methods relate to how we sample possible model architecture candidates from the space of possible hyperparameter values. This is often referred to as \"searching\" the hyperparameter space for the optimum values. In the following visualization, the   and   dimensions represent two hyperparameters, and the   dimension represents the model's score (defined by some evaluation metric) for the architecture defined by \n",
    " and \n",
    ".\n",
    "\n",
    "there are two generic approaches to search effectively in the HP space are GridSearch CV and RandomSearch CV. Here CV denotes Cross-Validation\n",
    "\n",
    "\n",
    "\n",
    "#### Grid search\n",
    "Grid search is arguably the most basic hyperparameter tuning method. With this technique, we simply build a model for each possible combination of all of the hyperparameter values provided, evaluating each model, and selecting the architecture which produces the best results.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149b526",
   "metadata": {},
   "source": [
    "Exploratory data analysis \n",
    "EDA helps dimensionality oreductrions vis better understand go f relationships between features. \n",
    " \n",
    " \n",
    "\n",
    "Data preprocessings Feature engineering,\n",
    "We add lags to the data that willbe sued in future steps, also dummy for the day and month of the year to xcapture seasonal effects\n",
    "additioally adding ad hoc dummy to  spsecifc shock event ssuchs as the pandmice,s  \n",
    "\n",
    " \n",
    "2 Perfome EXHAUSTIVE Feature ENGINEERINGS\n",
    "Fe DETAILED ICLUDING THE LSITINGS OF DERIVED features and specifications of the target label. \n",
    "Categorize near extremely zero resutls retins/. Drop form samples\n",
    "\n",
    "\n",
    "stacking models involve at elast three base learners\n",
    "hyperparameters optimized\n",
    "type of base learners and meta model to be used are design choices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ffcb8d",
   "metadata": {},
   "source": [
    "### Importing Package <a id=\"importing_Package\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing Packages\") \n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import scipy.stats as stats0\n",
    "from scipy.stats import norm as norm0\n",
    "import math as math0\n",
    "from tabulate import tabulate  \n",
    "import sympy as sp\n",
    "import scipy.stats as si\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas_datareader.data as web\n",
    "from pandas.tseries.offsets import BDay\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a23ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"scikit SKLEARN Packages\") \n",
    "#import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "#References Mahmudul Hasan \" A blening ensemble learning model for crude oil pirce prediciont\" 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#worked !pip install scikit-learn\n",
    "#worked #!pip install -U pytest\n",
    "#worked  #!pip install --upgrade twisted\n",
    "#worked  !pip install -U attrs\n",
    "#worked  !pip uninstall attrs\n",
    "#worked  !pip install attrs==19.1\n",
    "#worked  !pip install attrs==19.2.0 -i http://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec699274",
   "metadata": {},
   "source": [
    "## Feature Engineering <a id=\"feature\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff8b1f",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)\n",
    "    \n",
    "\n",
    "- Description Variables:\n",
    "- Curves\n",
    "-b utterfliues\n",
    "- rALY OR SELLF OF OF RATES\n",
    "- cURVATURE OF RATES p\n",
    "= pOlicty rates\n",
    "Rates expectiaotns\n",
    "\n",
    "-**Economcis variables:**\n",
    "\n",
    "- ***Prices: Headline Monthly Inflation:***\n",
    "- ***Prices: Core Montlhy inflaitn***\n",
    "- ***Prices: Stick Atalnada ***\n",
    "- ***Prices: Core PCE***\n",
    "- ***Prices: cLEVELD MEDINA pce****\n",
    "We process the infaltion in a mont on month bases, 3mont s on 3 months bassis annualziesd and 6month to month change bassis annulaize and year oon eyar bassis. \n",
    "Addiotnal y we add the \n",
    "- ***Real Sector: Retail Sales excluding Auto Sales***\n",
    "- ***Real Sector: GDP****\n",
    "- ***Real Sector: Household conusmption***\n",
    "- ***Real Sector: s&P Monthly GDP DATA***\n",
    "- *** Real Sector: Industrial Activity***\n",
    "- ***Cycle sector: PMI***\n",
    "- ***Cycle sector: Mchigan Conusmer Survey***\n",
    "- Cycle sector: Conference COnusmer Board\n",
    "- Cycle sector: Chicahgo Business\n",
    "- Cycle sector: ISM MANUFACTURING\n",
    "- Cycle sector: ism serives\n",
    "- Employment Sector: iNITAL jOBLESS cLIMAS\n",
    "- Employment Sector: cONTINEUOIS JIBLESS cLAIMS \n",
    "- Employment Sector: jOLTS hIRING RATES\n",
    "- Employment Sector: - Employment Sector: Unemployment\n",
    "- Employment Sector: Particiaptions\n",
    "- Employment Sector: NFP\n",
    "- Employment Sector: Wages growth\n",
    "\n",
    "Citi economics surprise index\n",
    "FED FOMC members Sentiment indicator: Hawkshness or Dobvishness meter\n",
    "Macro moentum score thta is  a weigthed index of a set of economic varaibles to capture the economic momentum \n",
    "Financial Variables:\n",
    "SPX 500 Index: US equityt market \n",
    "ftse\n",
    "Dax\n",
    "DXY usd dollar\n",
    "Vix \n",
    "ECB Europe rates\n",
    "BOE uk rates\n",
    "\n",
    "Ad hoc variables:\n",
    "FOMC mmetigns\n",
    "Calendar Seasonaity: Dummys for day of the week, day of the month, month, quarter, year\n",
    "Specifc adhoc for certain  events (Covid pandemic)\n",
    "Additionally we add different itnerations of moving averages,  scaled to zscore to differnet time windwos and also in differnt lags. \n",
    "Also histicall max and min rollig of last 30 dasys, 60 days, 90 days 160 days, Rollign averages, \n",
    "z score\n",
    "-Dependetna variabler to forecasts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- [Top](#table_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ccdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value=2022   \n",
    "#C:\\Alan_UK19\\2022 CQF\\2023 Project\\Data\n",
    "#https://github.com/ilchen/US_Economic_Data_Analysis/blob/main/CPI_and_Fed_Funds_Rates.ipynb\n",
    "print(\"load dataset\")\n",
    "#df = pd.read_csv('C:/Alan_UK19/2022 CQF/2023 Project/Data/dataset.csv', index_col=0, parse_dates=True)\n",
    "df = pd.read_csv('C:/Alan_UK19/2022 CQF/2023 Project/Data/dataset.csv', )\n",
    "df = df.iloc[::-1]\n",
    "df = df.set_index('Date')\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "#df = df.dropna()\n",
    "#df = df.dropna(how='all')\n",
    "#dropt na and drop duplicates\n",
    "#df=df.tail(1900)\n",
    "df_original=df.copy()\n",
    "print(df.head(4))\n",
    "print(df.shape[0])\n",
    "df.columns = df.columns.str.lower()\n",
    "df.index = pd.to_datetime(df.index, origin='1899-12-30', unit='D')\n",
    "df['date0']=df.index\n",
    "#df['date_m']=df['date0'].dt.month\n",
    "#df['date_y']=df['date0'].dt.year\n",
    "#df['date_d']=df['date0'].dt.day\n",
    "#df['month'] = df.index.datetime.month\n",
    "#3. Cleaning the Data\n",
    "#Check for:#\n",
    "#Missing Values (Imputation)\n",
    "#Any questionable values from the Pandas Profiling Report obtained above\n",
    "#%%time\n",
    "#print(model_df['Return_Target'].value_counts())\n",
    "#model_df.isna().sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Seasonality dummy\")\n",
    "### Create dummy of monht,year. day. \n",
    "# Create dummy columns for each month, day, and year\n",
    "print(\"Month\")\n",
    "df_d=pd.get_dummies(df.index.month, prefix='Month')\n",
    "df_d.index=df.index\n",
    "df_d_l0 = df_d.columns.tolist()\n",
    "print(df_d.columns.tolist())\n",
    "df = pd.concat([df, df_d],axis=1, sort=False)\n",
    "print(\"Daily\")\n",
    "df_d=pd.get_dummies(df.index.day, prefix='Day')\n",
    "df_d.index=df.index\n",
    "df_d_l1 = df_d.columns.tolist()\n",
    "df_d_l0 = df_d_l0+df_d_l1\n",
    "print(df_d.columns.tolist())\n",
    "df = pd.concat([df, df_d],axis=1, sort=False)\n",
    "\n",
    "print(\"Daily\")\n",
    "df_d=pd.get_dummies(df.index.dayofweek, prefix='Day_of_Week')\n",
    "df_d.index=df.index\n",
    "df_d_l1 = df_d.columns.tolist()\n",
    "df_d_l0 = df_d_l0+df_d_l1\n",
    "print(df_d.columns.tolist())\n",
    "df = pd.concat([df, df_d],axis=1, sort=False)\n",
    "print(df_d.columns.tolist())\n",
    "\n",
    "print(\"Year\")\n",
    "df_d=pd.get_dummies(df.index.year, prefix='Year')\n",
    "df_d.index=df.index\n",
    "df_d_l1 = df_d.columns.tolist()\n",
    "df_d_l0 = df_d_l0+df_d_l1\n",
    "print(df_d.columns.tolist())\n",
    "df = pd.concat([df, df_d],axis=1, sort=False) \n",
    "\n",
    "print(df_d.columns.tolist())\n",
    "print(df_d_l0)       \n",
    "#print(df.columns.tolist())\n",
    "#df_bu0002=df.copy()\n",
    "\n",
    "\n",
    "#Treating Categorical Variables\n",
    "#Actually it's a challenge to treat categorical variables\n",
    "#If we choose get_dummies it would make the separate columns according to the attributes uniquness and if the data is present it consider it as 1 and if it is not then 0.\n",
    "#We will endup with lot of attributes.\n",
    "#Treating each column with importance is tough.\n",
    "#Then there are other techniques label encoder Vs One hot encoding.\n",
    "\n",
    "#df = pd.get_dummies(data = df, columns = ['job'] , prefix = ['job'] )\n",
    "#df = pd.get_dummies(data = df, columns = ['marital'] , prefix = ['marital'] )\n",
    "\n",
    "\n",
    "\n",
    "print(\"Establishing date index\")\n",
    "#df[\"date\"]=df.index\n",
    "#df[\"year\"]=df.index.year\n",
    "#df[\"month\"]=df.index.month\n",
    "#df[\"day\"]=df.index.day\n",
    "#df[\"datetime_index\"]= pd.to_datetime(dict(year=df.year, month=df.month, day=df.day))\n",
    "#df['Date1'] = pd.to_datetime(pd.to_datetime(df['date']).apply(lambda x:x.strftime('%Y-%m-%d')))\n",
    "print(\"Size\")\n",
    "#df.shape\n",
    "print(\"Reviewing that there  are no Null values\")\n",
    "#df.isnull().sum() \n",
    "#print(df.columns)\n",
    "print(\"Graph of adj Close price of last 5 years\")\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot(df['adj close'])\n",
    "#plt.title('ISHARES RUSSELL 2000 ETF Daily Adjusted Close Price');\n",
    "########\n",
    "print(\"Creating Curves\")\n",
    "df['6ms1s']=df['6 mo']-df['1 yr']\n",
    "df['2s1s']=df['2 yr']-df['1 yr']\n",
    "df['3s1s']=df['3 yr']-df['1 yr']\n",
    "df['5s1s']=df['5 yr']-df['1 yr']\n",
    "#df['7s1s']=df['7 yr']-df['1 yr']\n",
    "#df['10s1s']=df['10 yr']-df['1 yr']\n",
    "print(\"Creating Curves\")\n",
    "###########\n",
    "df['3s2s']=df['3 yr']-df['2 yr']\n",
    "df['5s2s']=df['5 yr']-df['2 yr']\n",
    "df['7s2s']=df['7 yr']-df['2 yr']\n",
    "df['10s2s']=df['10 yr']-df['2 yr']\n",
    "###########\n",
    "print(\"Creating Curves\")\n",
    "df['5s3s']=df['5 yr']-df['3 yr']\n",
    "df['7s3s']=df['7 yr']-df['3 yr']\n",
    "df['10s3s']=df['10 yr']-df['3 yr']\n",
    "############\n",
    "print(\"Creating Curves\")\n",
    "df['7s5s']=df['7 yr']-df['5 yr']\n",
    "df['10s5s']=df['10 yr']-df['5 yr']\n",
    "\n",
    "\n",
    "######\n",
    "####Create \n",
    "print(\"List of columns before loop\")\n",
    "#### Creat lags of varaibles \n",
    "#'7s1s','10s1s',\n",
    "lag_variables = ['6ms1s','2s1s','3s1s','5s1s',\n",
    "                 '3s2s','5s2s','7s2s','10s2s',\n",
    "                 '5s3s','7s3s','10s3s',\n",
    "                 '7s5s','10s5s']\n",
    "# Replace 'max_lag' with the maximum number of lags you want to create\n",
    "\n",
    "\n",
    "print(\"Creating daily differential\")\n",
    "list_variable_names1 = []\n",
    "# Loop to generate lagged columns\n",
    "for variable in lag_variables:\n",
    "    new_column_name = f\"{variable}_diff\"\n",
    "    df[new_column_name] = df[variable].diff()\n",
    "    list_variable_names1.append(new_column_name)\n",
    "\n",
    "\n",
    "    \n",
    "list_endog =   lag_variables+list_variable_names1\n",
    "print(list_endog)\n",
    "#table_contents\n",
    "### COncatenating list of indeopendent variables \n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.shape[1])\n",
    "\n",
    "df_bu0001 =df.copy()\n",
    "#df['Date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "#df.set_index('date', inplace=True)\n",
    "\n",
    "# Convert the index to datetime format\n",
    "#df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Create dummy variables for each month\n",
    "#df['month'] = df.index.dt.month\n",
    "#df = pd.get_dummies(df, columns=['month'], prefix='month', drop_first=True)\n",
    "\n",
    "#print(df)\n",
    "# Create dummy variables for each month\n",
    "#df = pd.get_dummies(df, columns=['Date'], prefix='month', drop_first=True)\n",
    "#df.to_clipboard()\n",
    "# Dis\n",
    "\n",
    "#data.dtypes\n",
    "#df.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25caf3",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"load exogenous dataset: economic data\")\n",
    "#df = pd.read_csv('C:/Alan_UK19/2022 CQF/2023 Project/Data/dataset.csv', index_col=0, parse_dates=True)\n",
    "df_exog_eco = pd.read_csv('C:/Alan_UK19/2022 CQF/2023 Project/Data/dataset_exog_eco.csv', )\n",
    "#df_exog_eco = df_exog_eco.iloc[::-1]\n",
    "df_exog_eco = df_exog_eco.set_index('Date')\n",
    "df_exog_eco = df_exog_eco.apply(pd.to_numeric, errors='coerce')\n",
    "df_exog_eco.columns = df_exog_eco.columns.str.lower()\n",
    "df_exog_eco.index = pd.to_datetime(df_exog_eco.index, origin='1899-12-30', unit='D')\n",
    "#list_exog_eco=df_exog_eco.columns.tolist()\n",
    "\n",
    "\n",
    "\n",
    "list_exo_f =[\n",
    "    'nfp_diff','private_nfp_diff',\n",
    "'nfp_diff_rm','nfp_diff_sd','nfp_diff_zscore5y',\n",
    "'private_nfp_diff_rm','private_nfp_diff_sd','private_nfp_diff_zscore5y',\n",
    "'participation_rate',\n",
    "    'ur',\n",
    "    'ur_rm','ur_sd','ur_zscore5y',\n",
    "    'michigan_consumer',\n",
    "'michigan_consumer_rm','michigan_consumer_sd','michigan_consumer_zscore5y',\n",
    "    'chicago_activity',\n",
    "'chicago_activity_rm','chicago_activity_sd','chicago_activity_zscore5y',\n",
    "'industrial_prod_rm','industrial_prod_sd','industrial_prod_zscore5y',\n",
    "'retail_sales_rm','retail_sales_sd','retail_sales_zscore5y',\n",
    "'retail_sales_yy','retail_sales_6m_ann','retail_sales_3m_ann','retail_sales_mm',\n",
    "'industrial_prod_yy','industrial_prod_6m_ann','industrial_prod_3m_ann','industrial_prod_mm',\n",
    "'headline_cpi_yy','headline_cpi_6m_ann','headline_cpi_3m_ann','headline_cpi_mm',\n",
    "'core_cpi_yy','core_cpi_6m_ann','core_cpi_3m_ann','core_cpi_mm',\n",
    "'pce_deflator_yy','pce_deflator_6m_ann','pce_deflator_3m_ann','pce_deflator_mm',\n",
    "'core_pce_deflator_yy','core_pce_deflator_6m_ann','core_pce_deflator_3m_ann','core_pce_deflator_mm',\n",
    "'cleveland_median_pce']\n",
    "df_exog_eco=df_exog_eco[list_exo_f]\n",
    "list_exog_eco=df_exog_eco.columns.tolist()\n",
    "print(list_exog_eco)\n",
    "print(df_exog_eco.shape[0])\n",
    "print(df_exog_eco.shape[1])\n",
    "\n",
    "#Date\tretail_sales\tindustrial_prod\theadline_cpi\t\n",
    "#core_cpi\tpce_deflator\tcore_pce_deflator\t\n",
    "#michigan_consumer\tchicago_activity\tur\t\n",
    "#Labor Force Participation Rate\tNonfarm Payrolls\t\n",
    "#Private Nonfarm Payrolls\t\n",
    "#PCE Core Services Excluding Housing\t\n",
    "#FRB Cleveland Median PCE\t\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10085b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"load exogenous dataset: finance data\")\n",
    "\n",
    "df_exog_fin = pd.read_csv('C:/Alan_UK19/2022 CQF/2023 Project/Data/dataset_exog_fin.csv', )\n",
    "#df_exog_eco = df_exog_eco.iloc[::-1]\n",
    "df_exog_fin = df_exog_fin.set_index('Date')\n",
    "df_exog_fin = df_exog_fin.apply(pd.to_numeric, errors='coerce')\n",
    "df_exog_fin.columns = df_exog_fin.columns.str.lower()\n",
    "df_exog_fin.index = pd.to_datetime(df_exog_fin.index, origin='1899-12-30', unit='D')\n",
    "list_exog_fin=df_exog_fin.columns.tolist()\n",
    "print(list_exog_fin)\n",
    "print(df_exog_fin.shape[0])\n",
    "print(df_exog_fin.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matching the 3 datasets: rates, economic data, financial data\")\n",
    "df_complete = pd.merge(df, df_exog_eco, left_index=True, right_index=True, how='left',)\n",
    "df_complete = pd.merge(df_complete, df_exog_fin, left_index=True, right_index=True, how='left',)\n",
    "del df\n",
    "del df_exog_eco\n",
    "del df_exog_fin\n",
    "print(df_complete.shape[0])\n",
    "print(df_complete.shape[1])\n",
    "df_complete_list=list_endog+list_exog_eco+list_exog_fin\n",
    "print(df_complete_list)\n",
    "original_count = len(df_complete_list)\n",
    "print(len(df_complete_list))\n",
    "df_complete_list = list(set(df_complete_list))\n",
    "print(len(df_complete_list))\n",
    "\n",
    "\n",
    "df_complete_list_markets=list_endog+list_exog_fin\n",
    "print(df_complete_list_markets)\n",
    "original_count = len(df_complete_list_markets)\n",
    "print(len(df_complete_list_markets))\n",
    "df_complete_list_markets = list(set(df_complete_list_markets))\n",
    "print(len(df_complete_list_markets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bu0001=df.copy()\n",
    "#df_complete.to_clipboard()\n",
    "#df_d=pd.get_dummies(df.index.month, prefix='Month')\n",
    "#print(df_d)\n",
    "#df_complete.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14460945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_complete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d697233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb51f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02182e0",
   "metadata": {},
   "source": [
    "4. Feature Engineering¶\n",
    "This step consists of the following ideas:\n",
    "\n",
    "FeatureTools Generation\n",
    "Using FeatureTools, I will generate time series features for \n",
    "predicting the Return Target. \n",
    "TSFresh can be used as one way to do feature selection, \n",
    "the other being using Dense layers stacked on top of LSTM layers to reduce features. \n",
    "Therefore, 2 different datasets will be used for this project: (1) Full Feature Set and (2) TSFresh Reduced Dataset.\n",
    "\n",
    "Ordinal Encoding\n",
    "Categorical values will be encoded using ordinal encoding, so they can be processed as numbers. \n",
    "Note: Given that the dataset is not shuffled, ordinal encoding will be applied before train test and split, so there are no missing values. Otherwise, the test set date data will be encoded as \"Missing\" if the encoder does not run on the full dataset.\n",
    "\n",
    "Standard Scaling\n",
    "Given that we are working with LSTM's (deep learning models), \n",
    "I will standard scale the values so that this not affect training. The scaler will be first fit to the training data, and then applied on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e0986",
   "metadata": {},
   "source": [
    "### Technical Indicators <a id=\"technical_indicators\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe65bf",
   "metadata": {},
   "source": [
    "\n",
    "The technical indicators are:\n",
    "Momentum;\n",
    "lagged logarithmic return;\n",
    "return sign.\n",
    "exponential moving average (EMA);\n",
    "relative strenght index (RSI);\n",
    "stochastic oscilator (Stochastic K);\n",
    "moving average convergence divergence (MACD);\n",
    "commodity channel cndex (CCI);\n",
    "average true range (ATR);\n",
    "bollinger bands (BB);\n",
    "For every technical indicator, we create a well-documented function. After functions are introduced, we generate variables based on various windows.\n",
    "\n",
    "\n",
    "\n",
    "**MAV Moving Average** \n",
    "\n",
    "#### **EWMA- Exponentially Weighted Moving Average**\n",
    "- It is an enhancement of the traditional moving average that assigns exponentially decreasing weights to historical observations. The purpose of using EWMA is to give more weight to recent observations while still considering older data points, providing a smoother and more responsive measure of trends and variations over time.\n",
    " \n",
    "- EWMA Characteristics:\n",
    "- ***Volatility Estimation***: EWMA is commonly used in finance for estimating volatility. By applying EWMA to historical returns or price data, analysts can calculate a volatility measure that responds quickly to recent market conditions.\n",
    "\n",
    "- ***Exponential Decay***: The term \"exponential\" in EWMA reflects the fact that the weights assigned to older observations decrease exponentially, providing less influence on the current value.\n",
    "\n",
    "#### **Bollinger Bands**\n",
    "- **Bollinger Bands**: technical to assess the volatility and potential price movements. The primary purpose of Bollinger Bands is to provide a relative definition of high and low prices. The bands expand and contract based on the volatility of the market – they widen during more volatile periods and contract during less volatile periods Consist of three main components:\n",
    "    \n",
    "    - Middle Band (Simple Moving Average): The middle band is a moving average, typically \n",
    "    calculated over a specified period, such as 20 days\n",
    "    -Upper Band: The upper band is derived by adding a multiple of the standard deviation \n",
    "    of the asset's price to the middle band\n",
    "    -Upper Band: The upper band is derived by adding a multiple of the standard deviation \n",
    "    of the asset's price to the middle band\n",
    "   \n",
    "    Overbought and Oversold Conditions: When prices touch or exceed the upper band, the asset may be overbought, suggesting a potential reversal or pullback. Conversely, when prices touch or fall below the lower band, the asset may be oversold, indicating a potential rebound.\n",
    "\n",
    "    Volatility Squeeze: A period of low volatility is often indicated by the bands contracting closely together. Traders anticipate a potential price breakout when the bands suddenly expand.\n",
    "\n",
    "    Trend Confirmation: Bollinger Bands can be used to confirm the strength and direction of a trend. Prices consistently riding along the upper band may indicate a strong uptrend, while prices hugging the lower band may signify a strong downtrend.\n",
    "\n",
    "\n",
    "##### **MACD**\n",
    "**Moving Average Convergence Divergence, or MACD**, is a widely used technical analysis indicator that provides insights into the momentum and trend strength. Charactersitics: Trend Identification: Traders use the MACD to identify the overall trend direction of an asset. When the MACD line is above the Signal Line, it suggests a bullish trend, and when it is below, it indicates a bearish trend.\n",
    "\n",
    "    Momentum Confirmation: The MACD Histogram is used to confirm the strength of a price move. Rising histogram bars indicate increasing momentum, while declining bars suggest waning momentum.\n",
    "\n",
    "- MACD elements:\n",
    "- MACD Line (Fast Line): The MACD line is the core component and is calculated by subtracting the longer-term Exponential Moving Average (EMA) from the shorter-term EMA. The standard short-term period is 12 days, and the standard long-term period is 26 days.\n",
    "- Signal Line (Slow Line): The Signal Line is a 9-day EMA of the MACD line. It helps smooth out the MACD line, making it easier to identify potential trend reversals or shifts in momentum.\n",
    "- Histogram: The Histogram represents the difference between the MACD line and the Signal Line. It is plotted below the MACD chart and provides a visual representation of the divergence or convergence between the two lines. When the histogram is above zero, it indicates bullish momentum, while below zero suggests bearish momentum\n",
    "\n",
    "\n",
    "    Signal for Potential Reversals: Crossovers between the MACD line and the Signal Line can be used as signals for potential trend reversals. A bullish crossover (MACD line crossing above the Signal Line) may suggest a buying opportunity, while a bearish crossover (MACD line crossing below the Signal Line) may indicate a selling opportunity.\n",
    "\n",
    "    Divergence Analysis: Divergence between the MACD and the price chart can be a powerful signal. Bullish divergence occurs when prices make new lows, but the MACD does not confirm the new lows, suggesting a potential upward reversal. Conversely, bearish divergence occurs when prices make new highs, but the MACD does not confirm, indicating a potential downward reversal.\n",
    "\n",
    "\n",
    "#### **Relative Strength Index (RSI)**\n",
    "- The **Relative Strength Index (RSI)** is a popular momentum oscillator\n",
    "nterpretation:\n",
    "\n",
    "- Overbought Conditions: When the RSI surpasses the 70 level, it suggests that the asset may be overbought, and there is a higher likelihood of a potential reversal or correction in the price.\n",
    "- Oversold Conditions: Conversely, when the RSI falls below the 30 level, it indicates that the asset may be oversold, and there is a higher likelihood of a potential upward reversal or bounce in the price.\n",
    "    Divergence:\n",
    "\n",
    "- Bullish Divergence: Occurs when the price makes new lows, but the RSI fails to confirm those lows, creating a potential signal for a bullish reversal.\n",
    "    Bearish Divergence: Occurs when the price makes new highs, but the RSI fails to confirm those highs, signaling a potential bearish reversal.\n",
    "\n",
    "- Signal Confirmation:\n",
    "\n",
    "- Confirmation of Trends: The RSI can be used to confirm the strength of a trend. Rising RSI values during an uptrend and falling RSI values during a downtrend indicate a healthy trend.\n",
    "\n",
    "\n",
    "#### **Commodity Channel Index (CCI)**\n",
    "**Commodity Channel Index (CCI)** is a momentum-based technical indicato\n",
    " \n",
    " Overbought and Oversold Conditions:\n",
    "\n",
    "Overbought: When the CCI moves above +100, it indicates that the asset may be overextended to the upside, and there is a higher likelihood of a potential reversal or pullback.\n",
    "Oversold: Conversely, when the CCI falls below -100, it suggests that the asset may be oversold, and there is a higher likelihood of a potential upward reversal or bounce in the price.\n",
    "\n",
    "CCI divergence can provide potential signals for trend reversals. Bullish divergence occurs when prices make new lows, but the CCI does not confirm those lows. Bearish divergence occurs when prices make new highs, but the CCI fails to confirm those highs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d9bd9",
   "metadata": {},
   "source": [
    "### Technical Indicators <a id=\"technical\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a10637",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Technical Indicators: applying hte technical indicators to the dataset\")\n",
    "#target='y_10Y_USD'\n",
    "print(\"parameters\")\n",
    "rsi_period=14\n",
    "k_period=14\n",
    "d_period=3\n",
    "cci_period=30\n",
    "#y_tmpe=y_tmp.copy()\n",
    "#y_tmpe=pd.DataFrame (y_tmp@[target])\n",
    "#df=y_tmpe.copy()\n",
    "# Display the resulting DataFrame\n",
    "#print(df)\n",
    "#print(list_variable_names)\n",
    "#df_bu0001=df.copy()\n",
    "#df.to_clipboard()\n",
    "print(\"rates and market exog varaibles\")\n",
    "df_complete_list0=df_complete_list_markets\n",
    "df=df_complete.copy()\n",
    "for variable in df_complete_list0:\n",
    "    print(variable)\n",
    "    column_name = str(variable)\n",
    "    #print(\"rolling avg\")\n",
    "    #df[str(column_name)+'_rolling_avg5']=df [column_name].rolling (window=5).mean()\n",
    "    df[str(column_name)+'_rolling_avg10']=df [column_name].rolling (window=10).mean()\n",
    "    df[str(column_name)+'_rolling_avg22']=df [column_name].rolling (window=22).mean()\n",
    "    df[str(column_name)+'_rolling_avg44']=df[column_name].rolling (window=44) .mean()\n",
    "    #df[str(column_name)+'_rolling_avg66']=df [column_name].rolling (window=66) .mean()    \n",
    "    #print(\"rolling max\")\n",
    "    #df[str(column_name)+'_rolling_max5']=df [column_name].rolling (window=5).max()\n",
    "    df[str(column_name)+'_rolling_max10']=df [column_name].rolling (window=10).max()\n",
    "    df[str(column_name)+'_rolling_max22']=df[column_name].rolling (window=22).max()\n",
    "    df[str(column_name)+'_rolling_max44']=df [column_name].rolling (window=44) .max()\n",
    "    #df[str(column_name)+'_rolling_max66']=df[column_name].rolling (window=66) .max()\n",
    "    #print(\"rolling min\")\n",
    "    #df[str(column_name)+'_rolling_min5']=df[column_name].rolling (window=5).min()\n",
    "    df[str(column_name)+'_rolling_min10']=df [column_name].rolling (window=10).min()\n",
    "    df[str(column_name)+'_rolling_min22']=df[column_name].rolling (window=22).min()\n",
    "    df[str(column_name)+'_rolling_min44']=df [column_name].rolling (window=44).min()\n",
    "    #df[str(column_name)+'_rolling_min66']=df [column_name].rolling (window=66) .min()\n",
    "    #print(\"macd\")\n",
    "    df[str(column_name)+'_short_ema']=df[column_name].ewm(span=12, adjust=False).mean()\n",
    "    df[str(column_name)+'_long_ema']=df[column_name].ewm (span=26, adjust=False).mean()\n",
    "    df[str(column_name)+'_macd_line'] = df[str(column_name)+'_short_ema']-df[str(column_name)+'_long_ema']\n",
    "    df[str(column_name)+'_macd_line']=df[str(column_name)+'_short_ema']-df[str(column_name)+'_long_ema']\n",
    "    df[str(column_name)+'_signal_line'] = df[str(column_name)+'_macd_line'].ewm (span=9, adjust=False).mean()\n",
    "    df[str(column_name)+'_macd_histogram'] = df[str(column_name)+'_macd_line'].ewm (span=9, adjust=False).mean()\n",
    "    #df['k']=((df[target]-df[target].rolling (window-k_period).min())/(df [target].rolling (window-k_period).max()-df[target].rollir\n",
    "    #df['d']=df['k'].rolling (window=d_period).mean()\n",
    "    #print(\"Commodity Channel Index\")\n",
    "    df[str(column_name)+'_typical_price']=df[column_name]\n",
    "    #df[str(column_name)+'_mean_deviation']=abs (df[str(column_name)+'_typical_price'] -df[str(column_name)+'_typical_price'].rolling (window=cci_period).mean())\n",
    "    #df[str(column_name)+'_cci']=(df[str(column_name)+'_typical_price' ]-df[str(column_name)+'_typical_price'].rolling(window=cci_period).mean())/(0.015*df[str(column_name)+'_mean_deviation'].rolling(window=cci_period))\n",
    "    #### Bollinger Bands\n",
    "    df[str(column_name)+'_mavg20']=df[column_name].rolling (window=20).mean() \n",
    "    df[str(column_name)+'_std_dev20']=df[column_name].rolling (window=20).std()\n",
    "    df[str(column_name)+'_upper_band20']=df[str(column_name)+'_mavg20']+2*df[str(column_name)+'_std_dev20']\n",
    "    df[str(column_name)+'_lower_band20']=df[str(column_name)+'_mavg20' ]-2*df[str(column_name)+'_std_dev20']\n",
    "    #### Bollinger Bands\n",
    "    df[str(column_name)+'_mavg40']=df[column_name].rolling (window=40).mean() \n",
    "    df[str(column_name)+'_std_dev40']=df[column_name].rolling (window=40).std()\n",
    "    df[str(column_name)+'_upper_band40']=df[str(column_name)+'_mavg40']+2*df[str(column_name)+'_std_dev40']\n",
    "    df[str(column_name)+'_lower_band40']=df[str(column_name)+'_mavg40' ]-2*df[str(column_name)+'_std_dev40']\n",
    "    #### Bollinger Bands\n",
    "    df[str(column_name)+'_mavg60']=df[column_name].rolling (window=60).mean() \n",
    "    df[str(column_name)+'_std_dev60']=df[column_name].rolling (window=60).std()\n",
    "    df[str(column_name)+'_upper_band60']=df[str(column_name)+'_mavg60']+2*df[str(column_name)+'_std_dev60']\n",
    "    df[str(column_name)+'_lower_band60']=df[str(column_name)+'_mavg60' ]-2*df[str(column_name)+'_std_dev60']\n",
    "\n",
    "    ####rsi\n",
    "    df[str(column_name)+'_price_change']=df[column_name].diff()\n",
    "    df[str(column_name)+'_gain']=df[str(column_name)+'_price_change'].apply(lambda x: x if x>0 else 0) \n",
    "    df[str(column_name)+'_loss']=df[str(column_name)+'_price_change'].apply(lambda x:-x if x<0 else 0)\n",
    "    df[str(column_name)+'_avg_gain']=df[str(column_name)+'_gain'].rolling(window=rsi_period).mean() \n",
    "                                                                                                                                 \n",
    "    df[str(column_name)+'_avg_loss']=df[str(column_name)+'_loss'].rolling(window=rsi_period).mean() \n",
    "    df[str(column_name)+'_rs']=(df[str(column_name)+'_avg_gain']/df[str(column_name)+'_avg_loss'])\n",
    "    df[str(column_name)+'_rsi']=(100-(100/(1+df[str(column_name)+'_rs'])))\n",
    "\n",
    "    list_loop_l =[\n",
    "       str(column_name)+'_rolling_avg10', \n",
    "       str(column_name)+'_rolling_avg22',\n",
    "       str(column_name)+'_rolling_avg44', \n",
    "       str(column_name)+'_rolling_max10',\n",
    "       str(column_name)+'_rolling_max22',\n",
    "       str(column_name)+'_rolling_max44',\n",
    "       str(column_name)+'_rolling_min10',\n",
    "       str(column_name)+'_rolling_min22',\n",
    "       str(column_name)+'_rolling_min44', \n",
    "       str(column_name)+'_macd_histogram',\n",
    "       str(column_name)+'_upper_band20',\n",
    "       str(column_name)+'_lower_band20',\n",
    "       str(column_name)+'_upper_band40',\n",
    "       str(column_name)+'_lower_band40',\n",
    "       str(column_name)+'_upper_band60',\n",
    "       str(column_name)+'_lower_band60',\n",
    "       str(column_name)+'_rsi']\n",
    "    df_complete_list0 =df_complete_list0+list_loop_l   \n",
    "    del list_loop_l\n",
    "df_complete002=df.copy()  \n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_clipboard()\n",
    "#print(df_complete_list0)    \n",
    "#df_complete002.to_clipboard()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d77398",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating lags for rates and market variables (dont go above 20)\")\n",
    "max_lag = 5\n",
    "#list_variable_names2 = []\n",
    "# Loop to generate lagged columns\n",
    "for variable in df_complete_list0:\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        #print(variable)\n",
    "        #print(lag)\n",
    "        new_column_name = f\"{variable}_lag{lag}\"\n",
    "        #print(new_column_name)\n",
    "        df_complete002[new_column_name] = df_complete002[variable].shift(lag)\n",
    "        #list_variable_names2.append(new_column_name)\n",
    "                                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e79ef5",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "def scale_data(df_train:pd.DataFrame, df_test:pd.DataFrame, cat_features:list):\n",
    "    \"\"\"\n",
    "    Scales data using Standard Scaler, fit to the training data, and applied to\n",
    "    the test data.    \n",
    "    \"\"\"\n",
    "    num_features = df_train.columns.tolist()\n",
    "    for feature in cat_features:\n",
    "        num_features.remove(feature)\n",
    "    \n",
    "    df_train_scaled = df_train.copy(deep=True)\n",
    "    df_test_scaled = df_test.copy(deep=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_train_scaled[num_features] = scaler.fit_transform(df_train[num_features])\n",
    "    df_test_scaled[num_features] = scaler.transform(df_test[num_features])\n",
    "    \n",
    "    return df_train_scaled, df_test_scaled\n",
    "\n",
    "\n",
    "def encode_data(df:pd.DataFrame, cat_features:list):\n",
    "    \"\"\"\n",
    "    Encodes categorical variables using Ordinal Encoding\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy(deep=True)\n",
    "    encoder = OrdinalEncoder()\n",
    "    df_encoded[cat_features] = encoder.fit_transform(df[cat_features])\n",
    "\n",
    "    return df_encoded, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc94397",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)\n",
    "### Defining Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_complete002\n",
    "#df=df_bu0002.copy()\n",
    "# Traget variable \n",
    "print(\"Target variable\")\n",
    "#Calculaint dependent varaibel, dummy biinomial whne rates are rallyingh \n",
    "target='target_y'\n",
    "threshold=0.02\n",
    "df_complete002[target] = (df_complete002['2s1s_diff'] > threshold).astype(int)\n",
    "df_target = df_complete002[target] \n",
    "print(df_complete002[target])\n",
    "#df_target.to_clipboard()\n",
    "#df.to_clipboard()\n",
    "#df_bu0002=df.copy()\n",
    "#df_target1=df_target.dropna()\n",
    "#df_target1.to_clipboard()\n",
    "#print(df_target1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256f9f8",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a9673",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b192cb9",
   "metadata": {},
   "source": [
    "# distribution plots for each feature\n",
    "plt.figure(figsize=(25,20))\n",
    "k=1\n",
    "for i in range(0, len(df.columns)):\n",
    "    plt.subplot(4,4,k)\n",
    "    sns.histplot(data[df.columns[i]])\n",
    "    k+=1\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(15,15))\n",
    "plt.suptitle('Univariate Analysis of Numerical Features', fontsize=20, fontweight='bold', alpha=0.8, y=1.)\n",
    "\n",
    "for i in range(0, len(df_num)):\n",
    "    plt.subplot(5, 3, i+1)\n",
    "    sns.kdeplot(x = df[df_num[i]], shade=True, color='b')\n",
    "    plt.xlabel(df_num[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446295a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scatter plot to see the trend in each numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e2570",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.suptitle('Scatter plot for each numerical feature', fontsize=20, fontweight='bold', alpha=0.8, y=1.)\n",
    "for i in range(0, len(df_num)):\n",
    "    plt.subplot(5, 3, i+1)\n",
    "    sns.scatterplot(y=df_num[i], x=df.index, data=df)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c561f1",
   "metadata": {},
   "source": [
    "Correlation amongst predictors\n",
    "Lets consider the correlation heatmap below...\n",
    "\n",
    "print(\"Compute the correlation matrix among features\")\n",
    "corrFeatMatrix = df.corr()\n",
    "\n",
    "print(\"Plotting of features correlation matrix\")\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corrFeatMatrix, cmap=\"viridis\",annot=True)\n",
    "plt.title(\n",
    "    'Russell 2000: Correlation Matrix among features in the dataset', \n",
    "    fontsize=16, y=1.05, weight='bold'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25,20))\n",
    "sns.heatmap(df.corr(),cmap='RdBu', center=0, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad3898",
   "metadata": {},
   "source": [
    "X_selected = X_selected[selected2]\n",
    "X_selected.shape\n",
    "Features Describe\n",
    "X_selected.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a1aac",
   "metadata": {},
   "source": [
    "Correlation Matrix\n",
    "\n",
    "# first, let's compute the corr. matrix among features\n",
    "corrFeatMatrix = X_selected.corr()\n",
    "\n",
    "#Plotting of features correlation matrix\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corrFeatMatrix, cmap=\"viridis\",annot=True)\n",
    "# plt.title(\n",
    "#     'Correlation Matrix among features', \n",
    "#     fontsize=16, y=1.05, weight='bold'\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102de87",
   "metadata": {},
   "source": [
    "# setting correlation maximum threshold\n",
    "rho_max = 0.80\n",
    "\n",
    "# detecting pairwise features with abs(corr) > rho_max\n",
    "idxPairwise = np.where(abs(corrFeatMatrix) >= rho_max)\n",
    "lstPairwise = [\n",
    "    [corrFeatMatrix.index[x], corrFeatMatrix.columns[y], round(corrFeatMatrix.iloc[x, y], 3)] \n",
    "    for x, y in zip(*idxPairwise) if x != y and x < y\n",
    "]\n",
    "print(\"::>> Pairwise features with 𝜌_𝑚𝑎𝑥= +− 0.80 :\")\n",
    "print(\"::::>>>> List/list represents [Feat1, Feat2, Corr]:\")\n",
    "lstPairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37028d",
   "metadata": {},
   "source": [
    "Pair Plot\n",
    "sns.pairplot(X_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a41b98",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c209b9",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1364800",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3410e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68988897",
   "metadata": {},
   "source": [
    "print(\"Standardize or feature scaling the dataset\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#df_lasso=df.copy()\n",
    "#x_l1 = df_lasso[list_exog]\n",
    "#y_l1 = df_lasso[target]\n",
    "# Convert to binary outcome (1 for class 2, 0 for others)\n",
    "\n",
    "random_state_n=99\n",
    "\n",
    "print(\"# Split the data into training and testing sets\")\n",
    "# Split the data into training and testing sets\n",
    "#X_train_l1, X_test_l1, y_train_l1, y_test_l1 = train_test_split(x_l1, y_l1, test_size=0.2, random_state=random_state_n)\n",
    "\n",
    "# Standardize the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train_l1)\n",
    "#X_test_scaled = scaler.transform(X_test_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea95329",
   "metadata": {},
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regression = LinearRegression()\n",
    "regression\n",
    "\n",
    "\n",
    "regression.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "regression.coef_\n",
    "#Printing the coefficient\n",
    "regression.coef_\n",
    "\n",
    "#Printing the intercept\n",
    "regression.intercept_\n",
    "\n",
    "#Prediction for test data\n",
    "reg_pred = regression.predict(X_test)\n",
    "reg_pred\n",
    "\n",
    "\n",
    "# Residuals \n",
    "\n",
    "residuals = y_test - reg_pred\n",
    "residuals\n",
    "\n",
    "\n",
    "#Observation :\n",
    "#There is a negative correlation\n",
    "# performance Matrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_squared_error(y_test, reg_pred))\n",
    "print(mean_absolute_error(y_test, reg_pred))\n",
    "print(np.sqrt(mean_squared_error(y_test, reg_pred)))\n",
    "#R squared and Adjusted R squared\n",
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(y_test, reg_pred)\n",
    "print(score)\n",
    "\n",
    "print(\"Sca;ing the data\") \n",
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=3)\n",
    "scaler = StandardScaler()\n",
    "data_train=scaler.fit_transform(data_train)\n",
    "data_test=scaler.transform(data_test)\n",
    "data_train=pd.DataFrame(data_train, columns=data.columns)\n",
    "data_test=pd.DataFrame(data_test, columns=data.columns)\n",
    "xtrain=data_train.drop(labels='CSUSHPISA', axis=1)\n",
    "xtest=data_test.drop(labels='CSUSHPISA', axis=1)\n",
    "ytrain=data_train['CSUSHPISA']\n",
    "ytest=data_test['CSUSHPISA']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca926be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "#print(list_exog)\n",
    "\n",
    "\n",
    "# lsit of varaibles that will be used \n",
    "lag_columns = [column for column in df_complete002.columns if \"lag\" in column]\n",
    "d_columns = [column for column in df_complete002.columns if \"Day\" in column]\n",
    "m_columns = [column for column in df_complete002.columns if \"Month\" in column]\n",
    "y_columns = [column for column in df_complete002.columns if \"Year\" in column]\n",
    "feat_list=lag_columns+d_columns+m_columns+y_columns\n",
    "feat_list = [item for item in feat_list if \"inflation\" not in item]\n",
    "feat_list = feat_list + list_exo_f\n",
    "print(list_exo_f)\n",
    "target_y=['target_y']\n",
    "feat_list_y =feat_list + target_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c137de9",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8286fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "print(\"Cleaning and preparing the data\")\n",
    "\n",
    "#df_lr.to_clipboard()\n",
    "#Month_1\tMonth_2\tMonth_3\tMonth_4\tMonth_5\tMonth_6\tMonth_7\tMonth_8\tMonth_9\tMonth_10\tMonth_11\tMonth_12\tDay_1\tDay_2\tDay_3\tDay_4\tDay_5\tDay_6\tDay_7\tDay_8\tDay_9\tDay_10\tDay_11\tDay_12\tDay_13\tDay_14\tDay_15\tDay_16\tDay_17\tDay_18\tDay_19\tDay_20\tDay_21\tDay_22\tDay_23\tDay_24\tDay_25\tDay_26\tDay_27\tDay_28\tDay_29\tDay_30\tDay_31\tDay_of_Week_0\tDay_of_Week_1\tDay_of_Week_2\tDay_of_Week_3\tDay_of_Week_4\tYear_1970\tYear_2022\tYear_2023\n",
    "\n",
    "\n",
    "#x_lr.to_clipboard()\n",
    "\n",
    "\n",
    "\n",
    "#df_lr=df.copy()\n",
    "#df_lr=df_lr.tail(tail0)\n",
    "##df_lr = df_lr.dropna()\n",
    "##x_lr = df_lr[list_exog]\n",
    "#y_lr = df_lr[target]\n",
    "\n",
    "\n",
    "nan_mask.to_clipboard()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f80da2",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)\n",
    "\n",
    "### Enseble techniques\n",
    "\n",
    "- [z. Logistic regression](#logistic) \n",
    "- [Decision Tree Classifier](#dt) \n",
    "- [Decision Tree Classifier with HyperParameter Tuning](#dt_ht) \n",
    "- [Random Forest Classifier RF](#randomf) \n",
    "- [Random Forest Classifier Hyperparameter tuning : Random Forest Classifier](#randomf_ht) \n",
    "- [Extra Trees Classifier](#et) \n",
    "- [Extra Trees Classifier HyperParameter Tuning](#et_ht) \n",
    "- [Bagging Classifier](#bag) \n",
    "- [[Bagging Classifier with Hyperparameter tuning : Bagging Classifier](#bag_ht) \n",
    "- [Support Vector Machines Classifier ](#svc) \n",
    "- [LinearSVC - Lienar Support Vector Machines Classifier](#svcl) \n",
    "- [AdaBoostClassifier (#adaboost) ](#adaboost) \n",
    "- [GradientBoostingClassifier](#grad) \n",
    "- [XGBClassifier (XGBoost)](#xgb) \n",
    "- [CatBoostClassifier](#cat) \n",
    "- [LGBMClassifier (LightGBM)4.2.3 Light Gradient Boosting Machine Regression LGB learning rate](#lgbmc) \n",
    "\n",
    "- [ Voting Classifier](#voting) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06390bb",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pre steps\")\n",
    "df=df_complete002[feat_list_y]\n",
    "df=df.round(8)\n",
    "print(\"filtering years\")\n",
    "df = df[df.index.year >= 2009]\n",
    "df = df[df.index.year <= 2023]\n",
    "print(\"paramteres\")\n",
    "random_state0=99\n",
    "test_size0=0.2\n",
    "max_iter0=1000\n",
    "tail0=350\n",
    "#filter years\n",
    "#print(x_lr)\n",
    "#print(y_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b9027",
   "metadata": {},
   "source": [
    "### Logistic Regressions with Hyperparamteter Tuning \n",
    "\n",
    "LogisticRegression model. \n",
    "The hyperparameters such as C (inverse of regularization strength), penalty (type of regularization), and solver are tuned using GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc421e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regressions with Hyperparamteter Tuning \")\n",
    "print(\"from sklearn.linear_model import LogisticRegression\")\n",
    "\n",
    "df_lr=df.copy()\n",
    "#df_lr=df_lr.tail(tail0)\n",
    "#df_lr = df_lr.dropna()\n",
    "x_lr = df_lr[feat_list]\n",
    "y_lr = df_lr[target_y]\n",
    "nan_mask = x_lr.isna()\n",
    "print(x_lr.tail(1))\n",
    "print(y_lr.tail(1))\n",
    "\n",
    "print(\"Splitting Train and Test set\")\n",
    "x_lr_train, x_lr_test, y_lr_train, y_lr_test = train_test_split(x_lr,\n",
    "                                                                y_lr, \n",
    "                                                                test_size=test_size0, \n",
    "                                                                random_state=random_state0)\n",
    "\n",
    "print(\"Define the Logistic Regression model\")\n",
    "logreg_model = LogisticRegression(max_iter=max_iter0, \n",
    "                                  random_state=random_state0)\n",
    "\n",
    "\n",
    "print(\"Define hyperparameters for tuning\")\n",
    "param_grid = {'C': [0.1, 1, 10],\n",
    "              'penalty': ['l1', 'l2'],\n",
    "              'solver': ['liblinear']}\n",
    "\n",
    "print(\"Create GridSearchCV for hyperparameter tuning\")\n",
    "grid_search = GridSearchCV(estimator=logreg_model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(x_lr_train, y_lr_train)\n",
    "\n",
    "print(\"Get the best model from GridSearchCV\")\n",
    "best_logreg_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Make predictions on the test set with the best model\")\n",
    "y_pred = best_logreg_model.predict(x_lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03924f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate the Logistic Regression model\")\n",
    "accuracy = accuracy_score(y_lr_test, y_pred)\n",
    "print(f'Best Model Accuracy: {accuracy}')\n",
    "print(f'Best Model Parameters: {grid_search.best_params_}')\n",
    "print(\"Logistic Regressions with Hyperparamteter Tuning \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d789180",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_mask.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237df23",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)   \n",
    "       \n",
    "### Decision Tree Classifier\n",
    "\n",
    "- **Decision Tree** is a Supervised Machine Learning Algorithm that uses a set of rules to make decisions.\n",
    "- On every split, the algorithm tries to divide the dataset into the smallest subset possible[2]. \n",
    "- the goal is to minimize the loss function as much as possible.\n",
    "\n",
    " loss function that compares the class distribution before and after the split[2], like Gini Impurity and Entropy.\n",
    "\n",
    "- **Gini Impurity**: is a measure of variance across the different classe. \n",
    "- By default ScikitLearn uses Gini Impurity as a loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3f642",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Classifi\")\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "print(\"reference: from sklearn.tree import DecisionTreeClassifier\")\n",
    "\n",
    "df_dtc=df.copy()\n",
    "x_dtc = df_dtc[feat_list]\n",
    "y_dtc = df_dtc[target_y]\n",
    "print(\"Splitting\")\n",
    "x_dtc_train, x_dtc_test, y_dtc_train, y_dtc_test = train_test_split(x_dtc, y_dtc, \n",
    "                                                                    test_size=test_size0,\n",
    "                                                                    random_state=random_state0)\n",
    "print(x_dtc_train.shape, x_dtc_test.shape)\n",
    "dtc = DecisionTreeClassifier()\n",
    "print(\"By default ScikitLearn uses Gini Impurity as a loss function\")\n",
    "print(\"Fittign the model\")\n",
    "dtc.fit(x_dtc_train, y_dtc_train)\n",
    "model_dtc = DecisionTreeClassifier()\n",
    "model_dtc.fit(x_dtc_train, y_dtc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5678a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score\")\n",
    "print(\"from sklearn.metrics import accuracy_score\")\n",
    "print(\"Decision Tree Classifier training accuracy score is : {}%\".format(round(model_dtc.score(x_dtc_train, \n",
    "                                                                                               y_dtc_train)*100)))\n",
    "y_predict_dtc = model_dtc.predict(x_dtc_test)\n",
    "print(\"Decision Tree Classifier model's accuracy score is : {}%\".format(round(accuracy_score(y_dtc_test,\n",
    "                                                                                             y_predict_dtc)*100)))\n",
    "\n",
    "print(\"Roc-auc score \")\n",
    "print(\"from sklearn.metrics import roc_auc_score\")\n",
    "y_train_predict_roc = model_dtc.predict_proba(x_dtc_train)\n",
    "print(\"Decision Tree Classifier model's training roc-auc score is : {}%\".format(round(roc_auc_score(y_dtc_train, \n",
    "                                                                                                    y_train_predict_roc[:,1])*100)))\n",
    "y_test_predict_roc = model_dtc.predict_proba(x_dtc_test)\n",
    "print(\"Decision Tree Classifier model's roc-auc accuracy score is : {}%\".format(round(roc_auc_score(y_dtc_test,\n",
    "                                                                                                    y_test_predict_roc[:,1])*100)))\n",
    "\n",
    "print(\"Confusion_matrix\")\n",
    "print(\"from sklearn.metrics import confusion_matrix\")\n",
    "\n",
    "conf_mat = confusion_matrix(y_dtc_test, y_predict_dtc)\n",
    "conf_mat\n",
    "\n",
    "true_positive = conf_mat[0][0]\n",
    "false_positive = conf_mat[0][1]\n",
    "false_negative = conf_mat[1][0]\n",
    "true_negative = conf_mat[1][0]\n",
    "\n",
    "print(\"Confusion_matrix results\")\n",
    "print('True Positive:',\n",
    "      true_positive, \n",
    "      '\\nTrue Negative:',\n",
    "      true_negative, \n",
    "      '\\nFalse Negative:',\n",
    "      false_negative, \n",
    "      '\\nFalse Positive:',\n",
    "      false_positive)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(\"from sklearn.metrics import classification_report\")\n",
    "\n",
    "\n",
    "class_reprt_log_reg = classification_report(y_dtc_test, y_predict_dtc)\n",
    "print(class_reprt_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84507c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting Decision Tree\")\n",
    "# Plotting Decision Tree\n",
    "\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "tree.plot_tree(dtc, max_depth=5, filled=True, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a2872",
   "metadata": {},
   "source": [
    "The resutls of the Decision Tree Classifier are the fopllowing:\n",
    "The previsions was around 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60480b5",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)   \n",
    "       \n",
    "### Decision Tree Classifier with Hyper paramer Tuning\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Decision Tree Classifier with Hyper paramer Tuning \")\n",
    "print(\"Parameters\")\n",
    "grid_params = {\n",
    "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter' : ['best', 'random'],\n",
    "    'max_depth' : range(1,10,1),\n",
    "    'min_samples_split' : range(2,10,2),\n",
    "    'min_samples_leaf' : range(1,5,1),\n",
    "    'max_features' : ['auto', 'sqrt', 'log2']\n",
    "}   \n",
    "\n",
    "print(\"Grid Search\")\n",
    "grid_search_dtc = GridSearchCV(estimator= DecisionTreeClassifier(), \n",
    "                               param_grid=grid_params, \n",
    "                               verbose=2, \n",
    "                               n_jobs=-1, \n",
    "                               cv=3)\n",
    "print(\"using same  dataset\")\n",
    "grid_search_dtc.fit(x_dtc_train, y_dtc_train)\n",
    "print(\"Best Parameters\")\n",
    "grid_search.best_params_\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"Model with the best paramteres\")\n",
    "model_with_bst_prm_dtc = DecisionTreeClassifier(criterion = 'gini', \n",
    "                                                max_depth = 9, \n",
    "                                                min_samples_leaf = 1,\n",
    "                                                min_samples_split = 2)\n",
    "model_with_bst_prm_dtc.fit(X_train, y_train)\n",
    "print(\"Predicting witgh model with the best paramteres\")\n",
    "y_pred_bst_prm_dtc = model_with_bst_prm_dtc.predict(X_test)\n",
    "print (\"Decision Tree Classifier with Hyper paramer Tuning \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ecf054",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Decision Tree Classifier with Hyper paramer Tuning Accuracy Score\n",
    "print (\"Decision Tree Classifier with Hyperparameter Tuning \")\n",
    "Accuracy Score\n",
    "print(\"Accuracy Score\")\n",
    "# Decision Tree Regressor Model with best params accuracy score\n",
    "print(\"Decision Tree Regressor Model with best params training accuracy score is : {}%\".format(round(model_with_bst_prm_dtc.score(X_train, y_train)*100, 2)))\n",
    "print(\"Decision Tree Regressor Model with best params accuracy score is : {}%\".format(round(accuracy_score(y_test, y_pred_bst_prm_dtc)*100, 2)))\n",
    "\n",
    "print(\"Roc-auc score\")\n",
    "y_train_predict_roc_dtc_bst_prm = model_with_bst_prm_dtc.predict_proba(X_train)\n",
    "print(\"Decision Tree Regressor Model with best params training roc-auc score is : {}%\".format(round(roc_auc_score(y_train, y_train_predict_roc_dtc_bst_prm[:,1])*100)))\n",
    "y_test_predict_roc_dtc_bst_prm = model_with_bst_prm_dtc.predict_proba(X_test)\n",
    "print(\"Decision Tree Regressor Model with best params roc-auc accuracy score is : {}%\".format(round(roc_auc_score(y_test, y_test_predict_roc_dtc_bst_prm[:,1])*100)))\n",
    "\n",
    "print(\"Confusion_matrix\")\n",
    "conf_mat_dtc_bst_prm = confusion_matrix(y_test, y_pred_bc)\n",
    "conf_mat_dtc_bst_prm\n",
    "\n",
    "\n",
    "true_positive = conf_mat_dtc_bst_prm[0][0]\n",
    "false_positive = conf_mat_dtc_bst_prm[0][1]\n",
    "false_negative = conf_mat_dtc_bst_prm[1][0]\n",
    "true_negative = conf_mat_dtc_bst_prm[1][0]\n",
    "print(\"Confusion_matrix Results\")\n",
    "print('True Positive:',true_positive,\n",
    "      '\\nTrue Negative:',true_negative, \n",
    "      '\\nFalse Negative:',false_negative, \n",
    "      '\\nFalse Positive:',false_positive)\n",
    "print(\"Classification Report\")\n",
    "class_reprt_log_reg = classification_report(y_test, y_pred_bst_prm_dtc)\n",
    "print(class_reprt_log_reg)\n",
    "print (\"Decision Tree Classifier with Hyper paramer Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c10c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Decision Tree Classifier with Hyper paramer Tuning\")\n",
    "print(\"Plotting Decision Tree\")\n",
    "print(\"from sklearn import tree\")\n",
    "print(\" import matplotlib.pyplot as plt\")\n",
    "\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "tree.plot_tree(model_with_bst_prm_dtc, max_depth=9, filled=True, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbc4de",
   "metadata": {},
   "source": [
    "**Anlaysis of the resutls of the decision Tree Classifier with Hyper paramer Tuning:**\n",
    "- In comaprison witht he inital estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5fea5",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)  \n",
    "    \n",
    "# Random Forest Classifier  <a id=\"random\"></a>\n",
    "\n",
    "- **Random Forest Classifier** combines the output of multiple decision trees to reach a single result.\n",
    "- Random forests are for supervised machine learning, where there is a labeled target variable.\n",
    " - Random forests can be used for solving regression (numeric target variable) and classification (categorical target variable) problems.\n",
    " - Random forests are an ensemble method, meaning they combine predictions from other models.\n",
    " - Each of the smaller models in the random forest ensemble is a decision tree.\n",
    "- Random forest works on the Bagging principle. It creates a different training subset from sample training data with replacement & the final output is based on majority voting. For example,  Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier\")\n",
    "print(\"from sklearn.ensemble import RandomForestClassifier\")\n",
    "df_rfc=df.copy()\n",
    "x_rfc = df_rfc[feat_list]\n",
    "y_rfc = df_rfc[target_y]\n",
    "print(\"Splitting\")\n",
    "x_rfc_train, x_rfc_test, y_rfc_train, y_rfc_test = train_test_split(x_rfc, y_rfc, \n",
    "                                                                    test_size=test_size0,\n",
    "                                                                    random_state=random_state0)\n",
    "print(x_rfc_train.shape, x_rfc_test.shape)\n",
    "rfc = RandomForestClassifier()\n",
    "print(\"Fitting\")\n",
    "rfc.fit(x_rfc_train, y_rfc_train)\n",
    "print(\"Random Forest Classifier\")\n",
    "y_pred_rfc = rfc.predict(x_rfc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7420c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score\")\n",
    "# Random Forest Classifier Model's accuracy score\n",
    "print(\"Random Forest Classifier training accuracy score is : {}%\".format(\n",
    "    round(rfc.score(x_rfc_train, y_rfc_train)*100, 2)))\n",
    "print(\"Random Forest model's accuracy score is : {}%\".format(\n",
    "    round(accuracy_score(y_rfc_test, y_pred_rfc)*100, 2)))\n",
    "\n",
    "print(\"Roc-auc score\")\n",
    "y_train_predict_roc_rfc = rfc.predict_proba(x_rfc_train)\n",
    "print(\"Random Forest Classifier model with best parameter training training roc-auc score is : {}%\".format(\n",
    "    round(roc_auc_score(y_rfc_train, y_train_predict_roc_rfc[:,1])*100)))\n",
    "\n",
    "y_test_predict_roc_rfc = rfc.predict_proba(x_rfc_test)\n",
    "print(\"Random Forest Classifier model with best parameter training roc-auc accuracy score is : {}%\".format(\n",
    "    round(roc_auc_score(y_rfc_test, y_test_predict_roc_rfc[:,1])*100)))\n",
    "\n",
    "    \n",
    "print(\"Confusion_matrix\")\n",
    "conf_mat_rfc = confusion_matrix(y_rfc_test, y_pred_rfc)\n",
    "conf_mat_rfc\n",
    "\n",
    "true_positive = conf_mat_rfc[0][0]\n",
    "false_positive = conf_mat_rfc[0][1]\n",
    "false_negative = conf_mat_rfc[1][0]\n",
    "true_negative = conf_mat_rfc[1][0]\n",
    "\n",
    "print(\"Confusion_matrix Results\")\n",
    "print('True Positive:',true_positive, \n",
    "      '\\nTrue Negative:',true_negative, \n",
    "      '\\nFalse Negative:',false_negative, \n",
    "      '\\nFalse Positive:',false_positive)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "class_reprt_rfc = classification_report(y_rfc_test, y_pred_rfc)\n",
    "print(class_reprt_rfc)\n",
    "print(\"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f07232",
   "metadata": {},
   "source": [
    "**Analysis of Random Forest Classifier Model results:**\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e823f11",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)  \n",
    "    \n",
    "# Random Forest Classifier with Hyperparameter Tuning\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier with Hyperparameter Tuning\")\n",
    "\n",
    "\n",
    "grid_params = { 'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "               'max_depth' : range(1, 10, 1),\n",
    "               'min_samples_split' : range(2, 10, 2),\n",
    "               'min_samples_leaf' : range(1, 10, 1),\n",
    "}        \n",
    "        \n",
    "grid_search = GridSearchCV(estimator=rfc, \n",
    "                           param_grid=grid_params, \n",
    "                           n_jobs=-1, \n",
    "                           verbose=2, \n",
    "                           cv=3)        \n",
    "\n",
    "grid_search_rfc = grid_search.fit(X_train, y_train)\n",
    "print(\"Finding the best parameters\")\n",
    "grid_search_rfc.best_params_       \n",
    "        \n",
    "# default n-estimators value is 100\n",
    "\n",
    "model_with_bst_est_rfc = RandomForestClassifier(criterion = 'gini', \n",
    "                                                max_depth = 9, \n",
    "                                                min_samples_leaf = 1, \n",
    "                                                min_samples_split = 4, \n",
    "                                                verbose=1, \n",
    "                                                n_jobs=2)\n",
    "model_with_bst_est_rfc.fit(X_train, y_train)        \n",
    "y_pred_bst_est_rfc = model_with_bst_est_rfc.predict(X_test)   \n",
    "print(\"Random Forest Classifier with Hyperparameter Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0df5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier with Hyperparameter Tuning\")\n",
    "\n",
    "print(\"Accuracy Score\")\n",
    "# Random Forest Classifier model accuracy after hyper-parameter tuning\n",
    "\n",
    "print(\"Random Forest Classifier best parameter training accuracy score is : {}%\".format(round(model_with_bst_est_rfc.score(X_train, y_train)*100, 2)))\n",
    "print(\"Random Forest Classifier best parameter model's accuracy score is : {}%\".format(round(accuracy_score(y_test, y_pred_bst_est_rfc)*100, 2)))\n",
    "[Parallel\n",
    " \n",
    "print(\"Roc-auc score\")\n",
    "y_train_predict_roc_rfc_bst_est = model_with_bst_est_rfc.predict_proba(X_train)\n",
    "\n",
    "print(\"Random Forest Classifier model with best parameter training training roc-auc score is : {}%\".format(\n",
    "    round(roc_auc_score(y_train, y_train_predict_roc_rfc_bst_est[:,1])*100)))\n",
    "\n",
    "y_test_predict_roc_rfc_bst_est = model_with_bst_est_rfc.predict_proba(X_test)\n",
    "\n",
    "print(\"Random Forest Classifier model with best parameter training roc-auc accuracy score is : {}%\".format(\n",
    "    round(roc_auc_score(y_test, y_test_predict_roc_rfc_bst_est[:,1])*100)))\n",
    "\n",
    "    \n",
    "print(\"Confusion_matrix\")\n",
    "conf_mat_rfc_bst_est = confusion_matrix(y_test, y_pred_bst_est_rfc)\n",
    "conf_mat_rfc_bst_est\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "true_positive = conf_mat_rfc_bst_est[0][0]\n",
    "false_positive = conf_mat_rfc_bst_est[0][1]\n",
    "false_negative = conf_mat_rfc_bst_est[1][0]\n",
    "true_negative = conf_mat_rfc_bst_est[1][0]\n",
    "    \n",
    "print('True Positive:',true_positive, \n",
    "      '\\nTrue Negative:',true_negative, \n",
    "      '\\nFalse Negative:',false_negative, \n",
    "      '\\nFalse Positive:',false_positive)\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"Classification Report\")\n",
    "class_reprt_rfc_bst_prm = classification_report(y_test, y_pred_bst_est_rfc)\n",
    "print(class_reprt_rfc_bst_prm)\n",
    "    \n",
    "    \n",
    "print(\"Random Forest Classifier with Hyperparameter Tuning\")    \n",
    "print(\"Accuracy Score\")\n",
    "# Random Forest Classifier model accuracy after hyper-parameter tuning\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5bf68",
   "metadata": {},
   "source": [
    "**Anlysis of  Random Forest Classifier with Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7d956",
   "metadata": {},
   "source": [
    "   - [Top](#table_contents)   \n",
    "   \n",
    "### ExtraTrees  (Extremely Randomized Trees)\n",
    " \n",
    " **ExtraTrees** is an ensemble ML approach that trains numerous decision trees and aggregates the results from the group of decision trees to output a prediction. However, there are few differences between Extra Trees and Random Forest.\n",
    "  \n",
    "  Extra Trees uses the entire dataset to train decision trees. As such, to ensure sufficient differences between individual decision trees, it RANDOMLY SELECTS the values at which to split a feature and create child nodes\n",
    "  \n",
    "  \n",
    "  Using the entire dataset (which is the default setting and can be changed) allows ExtraTrees to reduce the bias of the model. However, the randomization of the feature value at which to split, increases the bias and variance. The paper that introduced the Extra Trees model conducts a bias-variance analysis of different tree based models. From the paper we see on most classification and regression tasks (six were analyzed) ExtraTrees have higher bias and lower variance than Random Forest. However, the paper goes on to say this is because the randomization in extra trees works to include irrelevant features into the model. As such, when irrelevant feature were excluded, say via a feature selection pre-modelling step, Extra Trees get a bias score similar to that of Random Forest.\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  Extra Tree Classifier (Extremely Randomized Trees)\n",
    "a type of ensemble learning technique which aggregates the results of multiple de-correlated decision\n",
    "trees collected in a \"forest\" to output it's classification result.\n",
    "In a certain way Decision Tree (High Variance), Random Forest (Medium Variance), Extra Trees (Low\n",
    "Variance)\n",
    "The extra trees algorithm, like the random forests algorithm, creates many decision trees, but the\n",
    "sampling for each tree is random, without replacement. This creates a dataset for each tree with\n",
    "unique samples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PocessS:\n",
    "Initially, selects a training subset randomly. It iteratively trains the AdaBoost machine learning model\n",
    "by selecting the training set based on the accurate prediction of the last training.\n",
    "It assigns the higher weight to wrong classified observations so that in the next iteration these\n",
    "observations will get the high probability for classification.\n",
    "Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the\n",
    "classifier. The more accurate classifier will get high weight.\n",
    "This process iterate until the complete training data fits without any error or until reached to the\n",
    "specified maximum number of estimators.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401c454",
   "metadata": {},
   "source": [
    "#### Extra Trees Classifier <a id=\"et\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c49b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Trees Classifier\")\n",
    "print(\"from sklearn.ensemble import ExtraTreesClassifier\")\n",
    "\n",
    "etc = ExtraTreesClassifier()\n",
    "etc.fit(X_train, y_train)\n",
    "y_pred_etc = etc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b93add",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Trees Classifier\")\n",
    "\n",
    "\n",
    "print(\"Accuracy Score\")\n",
    "# Random Forest Classifier model accuracy after hyper-parameter tuning\n",
    "\n",
    "print(\"Extra Trees Classifier training accuracy score is : {}%\".format(round(etc.score(X_train, y_train)*100, 2)))\n",
    "print(\"Extra Trees Classifier model's accuracy score is : {}%\".format(round(accuracy_score(y_test, y_pred_etc)*100, 2)))\n",
    "\n",
    "print(\"Roc-auc score\")\n",
    "y_train_predict_roc_etc = etc.predict_proba(X_train)\n",
    "\n",
    "print(\"Extra Trees Classifier model with best parameter training training roc-auc score is : {}%\".format(round(roc_auc_score(y_train, y_train_predict_roc_etc[:,1])*100)))\n",
    "\n",
    "y_test_predict_roc_etc = etc.predict_proba(X_test)\n",
    "\n",
    "print(\"Extra Trees Classifier model with best parameter training roc-auc accuracy score is : {}%\".format(round(roc_auc_score(y_test, y_test_predict_roc_etc[:,1])*100)))\n",
    " \n",
    "    \n",
    "print(\"Confusion_matrix\")\n",
    "conf_mat_etc = confusion_matrix(y_test, y_pred_etc)\n",
    "conf_mat_etc\n",
    "\n",
    "true_positive = conf_mat_etc[0][0]\n",
    "false_positive = conf_mat_etc[0][1]\n",
    "false_negative = conf_mat_etc[1][0]\n",
    "true_negative = conf_mat_etc[1][0]\n",
    "print('True Positive:',true_positive, \n",
    "      '\\nTrue Negative:',true_negative, \n",
    "      '\\nFalse Negative:',false_negative, \n",
    "      '\\nFalse Positive:',false_positive)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "class_reprt_etc= classification_report(y_test, y_pred_etc)\n",
    "print(class_reprt_etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e013e",
   "metadata": {},
   "source": [
    "#### Extra Trees Classifier with HyperParameter Tuning <a id=\"et_ht\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a55a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Trees Classifier with HyperParameter Tuning\")\n",
    "4.8 HyperParameter Tuning\n",
    "Extra Tree Classifier\n",
    "\n",
    "grid_params = {\n",
    "    'n_estimators' : [10,20,30],\n",
    "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth' : range(2,10,1),\n",
    "    'min_samples_split' : range(2,10,2),\n",
    "    'min_samples_leaf' : range(1,5,1),\n",
    "    'max_features' : ['sqrt', 'log2']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=ExtraTreesClassifier(), param_grid=grid_params, n_jobs=4, verbose=3, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "model_with_bst_prm_etc = ExtraTreesClassifier(criterion = 'log_loss',\n",
    "                                              max_depth = 9,\n",
    "                                              max_features = 'sqrt',\n",
    "                                              min_samples_leaf = 2,\n",
    "                                              min_samples_split = 8,\n",
    "                                              n_estimators = 10)\n",
    "\n",
    "model_with_bst_prm_etc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_etc_bst_prm = model_with_bst_prm_etc.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Confusion_matrix\")\n",
    "conf_mat_etc_bst_prm = confusion_matrix(y_test, y_pred_etc_bst_prm)\n",
    "conf_mat_etc_bst_prm\n",
    "\n",
    "\n",
    "true_positive = conf_mat_etc_bst_prm[0][0]\n",
    "false_positive = conf_mat_etc_bst_prm[0][1]\n",
    "false_negative = conf_mat_etc_bst_prm[1][0]\n",
    "true_negative = conf_mat_etc_bst_prm[1][0]\n",
    "print('True Positive:',true_positive, \n",
    "      '\\nTrue Negative:',true_negative, \n",
    "      '\\nFalse Negative:',false_negative,\n",
    "      '\\nFalse Positive:',false_positive)\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"Classification Report\")\n",
    "class_reprt_etc_bst_prm = classification_report(y_test, y_pred_etc_bst_prm)\n",
    "print(class_reprt_etc_bst_prm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64944c4",
   "metadata": {},
   "source": [
    "   - [Top](#table_contents)   \n",
    "\n",
    "## Bagging Classifier\n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea7a1a",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Bagging Classifier Hyerparametne Bootstrap aggregating) i\n",
    "technique for improving the accuracy of predictions made by a supervised learning algorithm\n",
    "Xpe of ensemble learning in which multiple base models are trained independently in parallel on\n",
    "different subsets of the training data. Each subset is generated using bootstrap sampling, in which\n",
    "data points are picked at random with replacement. In the case of the Bagging classifier, the final\n",
    "prediction is made by aggregating the predictions of the all-base model, using majority voting. In the\n",
    "case of regression, the final prediction is made by averaging the predictions of the all-base model,\n",
    "and that is known as bagging regression.\n",
    "The main advantage of bagging is that it can reduce the variance of the predictions made by a\n",
    "supervised learning algorithm without significantly compromising its accuracy. This makes it an\n",
    "attractive technique for problems where the cost of making a mistake is high\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0a535",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(\"  Bagging Classifier\") \n",
    "print(\"from sklearn.ensemble import BaggingClassifier\")\n",
    "\n",
    "bagg_cls = BaggingClassifier()\n",
    "       \n",
    "       # Model training\n",
    "\n",
    "bagg_cls.fit(X_train, y_train)\n",
    "       # Predicting values\n",
    "\n",
    "y_pred_bc = bagg_cls.predict(X_test)\n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9600df1",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(\"  Bagging Classifier\")      \n",
    "       \n",
    "5.1 Accuracy Score\n",
    "# Bagging Classifier Model's accuracy score\n",
    "\n",
    "print(\"Bagging Classifier training accuracy score is : {}%\".format(round(bagg_cls.score(X_train, y_train)*100, 2)))\n",
    "print(\"Bagging Classifier model's accuracy score is : {}%\".format(round(accuracy_score(y_test, y_pred_bc)*100, 2)))\n",
    "Bagging Classifier training accuracy score is : 99.14%\n",
    "Bagging Classifier model's accuracy score is : 91.79%\n",
    "Observation :\n",
    "This is an over-fitted model\n",
    "5.2 Roc-auc score\n",
    "y_train_predict_roc_bc = bagg_cls.predict_proba(X_train)\n",
    "\n",
    "print(\"Bagging Classifier model's training roc-auc score is : {}%\".format(round(roc_auc_score(y_train, y_train_predict_roc_bc[:,1])*100)))\n",
    "\n",
    "y_test_predict_roc_bc = bagg_cls.predict_proba(X_test)\n",
    "\n",
    "print(\"Bagging Classifier model's roc-auc accuracy score is : {}%\".format(round(roc_auc_score(y_test, y_test_predict_roc_bc[:,1])*100)))\n",
    "Bagging Classifier model's training roc-auc score is : 100%\n",
    "Bagging Classifier model's roc-auc accuracy score is : 94%\n",
    "5.3 Confusion_matrix\n",
    "conf_mat_bc = confusion_matrix(y_test, y_pred_bc)\n",
    "conf_mat_bc\n",
    "array([[21572,   840],\n",
    "       [ 1608,  5808]], dtype=int64)\n",
    "true_positive = conf_mat_bc[0][0]\n",
    "false_positive = conf_mat_bc[0][1]\n",
    "false_negative = conf_mat_bc[1][0]\n",
    "true_negative = conf_mat_bc[1][0]\n",
    "print('True Positive:',true_positive, '\\nTrue Negative:',true_negative, '\\nFalse Negative:',false_negative, '\\nFalse Positive:',false_positive)\n",
    "True Positive: 21572 \n",
    "True Negative: 1608 \n",
    "False Negative: 1608 \n",
    "False Positive: 840\n",
    "Classification Report\n",
    "class_reprt_log_reg = classification_report(y_test, y_pred_bc)\n",
    "print(class_reprt_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42983a6",
   "metadata": {},
   "source": [
    "   - [Top](#table_contents)  \n",
    "### Bagging ClassifierHyperparameter tuning\n",
    "       \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aac6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    4.4 Grid Search CV : Hyperparameter tuning\n",
    "print(\"Bagging Classifier with Hyperparameter Tuning\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining parameters for hyper parameters\n",
    "\n",
    "grid_params = {'n_estimators' : [5, 10, 15],\n",
    "        'max_samples' : range(2, 10, 1),\n",
    "        'max_features' : range(2, 10, 3)\n",
    "        }\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=bagg_cls, param_grid=grid_params, verbose=2, n_jobs=-1, cv=3)\n",
    "# Hyper parameter tuning\n",
    "\n",
    "grid_search_bc = grid_search.fit(X_train, y_train)\n",
    "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
    "# Finding the best parameters\n",
    "\n",
    "grid_search_bc.best_params_\n",
    "{'max_features': 5, 'max_samples': 8, 'n_estimators': 15}\n",
    "model_with_best_params_bc = BaggingClassifier(max_features = 5, max_samples = 7, n_estimators = 10, oob_score=True)\n",
    "model_with_best_params_bc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_bst_est_bc = model_with_best_params_bc.predict(X_test)\n",
    "print(\"Bagging Classifier with Hyperparameter Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d19924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bagging Classifier with Hyperparameter Tuning\")\n",
    "\n",
    "\n",
    "5.1 Accuracy Score\n",
    "# Bagging Classifier Model's accuracy score after hyper parameter tuning\n",
    "\n",
    "print(\"Bagging Classifier with best parameter training accuracy score is : {}%\".format(round(model_with_best_params_bc.score(X_train, y_train)*100, 2)))\n",
    "print(\"Bagging Classifier with best parameter model's accuracy score is : {}%\".format(round(accuracy_score(y_test, y_pred_bst_est_bc)*100, 2)))\n",
    "Ba\n",
    "\n",
    "\n",
    "5.2 Roc-auc score\n",
    "y_train_predict_bc_bst_prm = model_with_best_params_bc.predict_proba(X_train)\n",
    "\n",
    "print(\"Bagging Classifier model with best parameter training training roc-auc score is : {}%\".format(round(roc_auc_score(y_train, y_train_predict_bc_bst_prm[:,1])*100)))\n",
    "\n",
    "y_test_predict_roc_bc_bst_prm = model_with_best_params_bc.predict_proba(X_test)\n",
    "\n",
    "print(\"Bagging Classifier model with best parameter training roc-auc accuracy score is : {}%\".format(round(roc_auc_score(y_test, y_test_predict_roc_bc_bst_prm[:,1])*100)))\n",
    "Ba\n",
    "\n",
    "\n",
    "5.3 Confusion_matrix\n",
    "conf_mat_bc_bst_prm = confusion_matrix(y_test, y_pred_bc)\n",
    "conf_mat_bc_bst_prm\n",
    "array([[21572,   840],\n",
    "       [ 1608,  5808]], dtype=int64)\n",
    "true_positive = conf_mat_bc_bst_prm[0][0]\n",
    "false_positive = conf_mat_bc_bst_prm[0][1]\n",
    "false_negative = conf_mat_bc_bst_prm[1][0]\n",
    "true_negative = conf_mat_bc_bst_prm[1][0]\n",
    "print('True Positive:',true_positive, '\\nTrue Negative:',true_negative, '\\nFalse Negative:',false_negative, '\\nFalse Positive:',false_positive)\n",
    "True Positive: 2\n",
    "    \n",
    "Classification Report\n",
    "class_reprt_log_reg = classification_report(y_test, y_pred_bst_est_bc)\n",
    "print(class_reprt_log_reg)\n",
    "\n",
    "print(\"Bagging Classifier with Hyperparameter Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524aeac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "- [Top](#table_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cac68a",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM) model.\n",
    "\n",
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad4ea8",
   "metadata": {},
   "source": [
    "### How does varying C parameter affects the model's bias and variance trade-off?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "In a **Support Vector Machine (SVM), the regularization C (\"Cost\") parameter**  plays a crucial role in balancing the the trade-off between achieving a low training error and a low testing error (bias-variance trade-off). \n",
    "\n",
    "- The ***regularization cost parameter C*** controls the trade-off between maximizing the margin (minimizing the classification error by allowing margin violations) and minimizing the classification error on the training data. Consequently, it plays a significat role in the mathematical formulation of SVM and has important implications for the model's complexity and generalization performance.\n",
    "\n",
    "\n",
    "- The ***regularization cost parameter C*** is involved in the mathematical formulation of the optimization problem. The standard soft-margin SVM optimization problem can be written as follows:\n",
    "\n",
    "    - Minimize:\n",
    "    $$ 1/2 * ||w||^2 + C * Σξ $$\n",
    "\n",
    "    - Subject to:\n",
    "$$ yᵢ(wᵀxᵢ + b) ≥ 1 - ξᵢ,$$ \n",
    "$$ξᵢ ≥ 0,$$ \n",
    "\n",
    "   - In this formulation, C appears as a multiplier on the summation term (Σξ), representing the total margin violations. The optimization aims to minimize the magnitude of the weight vector (w) while controlling the influence of the cost parameter C on the margin violations (ξᵢ).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Lets do first a quick overview of  **SVM models**:\n",
    "\n",
    "- **Support Vector Machine (SVM) models** are tools for classification and regression tasks. A SVM model aims to find the optimal hyperplane that separates data points of different classes while maximizing the margin, which is the distance between the hyperplane and the nearest support vectors. SVM models can be fine-tuned using regularization parameters to control the trade-off between model complexity and generalization performance.\n",
    "\n",
    "- The main parameters are:\n",
    "    - **Regularization C  (Cost Parameter)**: primary regularization parameter that controls the trade-off between achieving a low training error and a low testing error.\n",
    "    - **Regularization Gamma paramter (γ)**: The gamma parameter defines how far the influence of a single training example reaches. It affects the shape of the decision boundary and can be considered a regularization parameter for models using the radial basis function (RBF) kernel.\n",
    "\n",
    "\n",
    "\n",
    "Comparing the effect of different regularizaiton C parameters values: (**How does varying C parameter affects the model's bias and variance trade-off?**)\n",
    "\n",
    "- Returning back to the **regularization C (\"Cost\") paramter**. A high C value leads to low bias but high variance, while a low C value leads to high bias but low variance. \n",
    "\n",
    "**Small C paramter (C < 1): allows for more margin violations (misclassifications) and leads to a simpler model with higher bias and lower variance**\n",
    "- When parameter C has a small value, the model places a higher priority in maximizing the margin. As a result the SVM model tolerates some degree of classification errors (misclassified training points) and therefore maintains a larger margin.\n",
    "- As a result, this leads to a simpler model with higher bias and lower variance.\n",
    "- The decision boundary may be more robust and less prone to overfitting but may result in some training errors.\n",
    "- A small C value encourages a wider margin, allowing for more margin violations (classification errors).\n",
    "\n",
    "\n",
    "**Large C paramter(C > 1): penalizes margin violations more heavily and results in a more complex model with lower bias but potentially higher variance.**\n",
    "\n",
    "- When C has a large value, it places a higher priority on classifying all training points correctly.\n",
    "- SVM penalizes margin violations (classification errors) more heavily, which results in a smaller margin.\n",
    "- A smaller margin leads to a more complex model that fits the training data closely.\n",
    "- This results in lower bias but potentially higher variance, making the model more prone to overfitting.\n",
    "- A large C value prioritizes accurate classification, even if it results in a smaller margin and fewer margin violations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SVM Regressor model (SVM regression algorithm is referred to as Support Vector Regression or SVR)\n",
    "Support Vector Regression is a supervised learning algorithm that is used to predict discrete values.\n",
    "basic idea behind SVR is to find the best fit line. In SVR, the best fit line is the hyperplane that has the\n",
    "maximum number of points.\n",
    "Linear SVR provides a faster implementation than SVR but only considers the linear kernel.\n",
    "regressor = SVR(kemel = 'rbf)\n",
    "Advantages of Support Vector Regression:\n",
    "1. It is robust to outlierst has excellent generalization capability, with high prediction accuracy.\n",
    "Disadvantages not suitable for large datasets. Decision model does not perform very well when the\n",
    "data set has more noise i.e. target classes are overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5149172",
   "metadata": {},
   "outputs": [],
   "source": [
    "Support Vector Machine classifier\n",
    "print(\"Support Vector Machine classifier\")\n",
    "\n",
    "\n",
    "# Define the Support Vector Machine classifier\n",
    "svm_classifier = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the SVM Classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Best Model Accuracy: {accuracy}')\n",
    "print(f'Best Model Parameters: {grid_search.best_params_}')\n",
    "\n",
    "print(\"Support Vector Machine classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5045490",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lineart SVC\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Linear Support Vector Machine model\n",
    "linear_svc_model = LinearSVC(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "linear_svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = linear_svc_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Linear Support Vector Machine model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbb407",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)\n",
    "\n",
    "### AdaBoostClassifier\n",
    "-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AdaBoostClassifier\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Split the data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y_binary, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Define the base classifier (DecisionTreeClassifier is commonly used)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)  # Shallow decision tree\n",
    "\n",
    "# Define the AdaBoostClassifier model\n",
    "adaboost_model = AdaBoostClassifier(base_classifier, random_state=42)\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=adaboost_model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_adaboost_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_pred = best_adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the AdaBoostClassifier model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Best Model Accuracy: {accuracy}')\n",
    "print(f'Best Model Parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5cf527",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2e0eb",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier\n",
    "\n",
    "**Gradient Boosting Classifiers**\n",
    "Gradient Boosting is a functional gradient algorithm that repeatedly selects a function that leads in the\n",
    "direction of a weak hypothesis or negative gradient so that it can minimize a loss function. Gradient\n",
    "boosting classifier combines several weak learning models to produce a powerful predicting model.\n",
    "The term \"Gradient\" in Gradient Boosting refers to the fact that you have two or more derivatives of\n",
    "the same function (we'll cover this in more detail later on). Gradient Boosting is an iterative functional\n",
    "gradient algorithm, ie an algorithm which minimizes a loss function by iteratively choosing a function\n",
    "that points towards the negative gradient; a weak hypothesis.\n",
    "\n",
    "- Gradient Boosting has three main components:\n",
    "Loss Function - The role of the loss function is to estimate how good the model is at making\n",
    "predictions with the given data. This could vary depending on the problem at hand. For\n",
    "example, if we're trying to predict the weight of a person depending on some input variables\n",
    "(a regression problem), then the loss function would be something that helps us find the\n",
    "difference between the predicted weights and the observed weights. On the other hand, if\n",
    "we're trying to categorize if a person will like a certain movie based on their personality, we'll\n",
    "require a loss function that helps us understand how accurate our model is at classifying\n",
    "people who did or didn't like certain movies.\n",
    "Weak Learner - A weak learner is one that classifies our data but does so poorly, perhaps no\n",
    "better than random guessing. In other words, it has a high error rate. These are\n",
    "      typically decision trees (also called decision stumps, because they are less complicated than\n",
    "typical decision trees).\n",
    "• Additive Model - This is the iterative and sequential approach of adding the trees (weak\n",
    "learners) one step at a time. After each iteration, we need to be closer to our final model. In\n",
    "other words, each iteration should reduce the value of our loss function\n",
    "Advantages and Disadvantages of Gradient Boost\n",
    "Advantages:\n",
    "Numerous choices for hyperparameter adjustment and the ability to optimize various loss functions.\n",
    "Disadvantages:\n",
    "Gradient Boosting classifier will keep getting better to reduce all inaccuracies. This may lead to\n",
    "overfitting and an overemphasis on outliers Less interpretative, even though this can be easily\n",
    "corrected with several tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87aefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "             \n",
    "                   \n",
    "print(\"GradientBoostingClassifier with Hyperparameter Tuning\")             \n",
    "                   \n",
    "print(\"Split the data into training and testing sets\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y_binary, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Define the GradientBoostingClassifier model\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_pred = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the GradientBoostingClassifier model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Best Model Accuracy: {accuracy}')\n",
    "print(f'Best Model Parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86172f",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd017c",
   "metadata": {},
   "source": [
    "## Ensemble Technique\n",
    "### Heterogeneous Ensemble Machine Learning Model using Voting Classifier\n",
    "\n",
    "In scikit-learn's VotingClassifier,\n",
    "you can use a variety of classifiers \n",
    "that implement the fit and predict methods. \n",
    "\n",
    "\n",
    "Note: When using VotingClassifier with hard voting, all classifiers must be able to predict class labels (i.e., they must have a predict method). When using soft voting, all classifiers must also have a predict_proba method for computing class probabilities.\n",
    "\n",
    "\n",
    "\n",
    "Here's a non-exhaustive list of classifiers that you can use:\n",
    "\n",
    "\n",
    "\n",
    "LogisticRegression\n",
    "RandomForestClassifier\n",
    "LinearSVC\n",
    "DecisionTreeClassifier\n",
    "Other:\n",
    "XGBClassifier (XGBoost)\n",
    "CatBoostClassifier\n",
    "LGBMClassifier (LightGBM)\n",
    "\n",
    "\n",
    "Ensemble Methods:\n",
    "\n",
    "RandomForestClassifier\n",
    "AdaBoostClassifier\n",
    "GradientBoostingClassifier\n",
    "BaggingClassifier\n",
    "\n",
    "Linear Models:\n",
    "LogisticRegression\n",
    "SGDClassifier\n",
    "\n",
    "\n",
    "Support Vector Machines:\n",
    "SVC\n",
    "LinearSVC\n",
    "\n",
    "Nearest Neighbors:\n",
    "KNeighborsClassifier\n",
    "\n",
    "Naive Bayes:\n",
    "GaussianNB\n",
    "MultinomialNB\n",
    "\n",
    "Decision Trees:\n",
    "DecisionTreeClassifier\n",
    "\n",
    "Neural Networks:\n",
    "MLPClassifier (Multi-layer Perceptron)\n",
    "\n",
    "Other:\n",
    "XGBClassifier (XGBoost)\n",
    "CatBoostClassifier\n",
    "LGBMClassifier (LightGBM)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class sklearn.ensemble.VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)[source]¶\n",
    "\n",
    "\n",
    "estimatorslist of (str, estimator) tuples\n",
    "Invoking the fit method on the VotingClassifier will fit clones of those original estimators that will be stored in the class attribute self.estimators_. An estimator can be set to 'drop' using set_params.\n",
    "\n",
    "Changed in version 0.21: 'drop' is accepted. Using None was deprecated in 0.22 and support was removed in 0.24.\n",
    "\n",
    "voting{‘hard’, ‘soft’}, default=’hard’\n",
    "If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n",
    "\n",
    "weightsarray-like of shape (n_classifiers,), default=None\n",
    "Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.\n",
    "\n",
    "n_jobsint, default=None\n",
    "The number of jobs to run in parallel for fit. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "\n",
    "New in version 0.18.\n",
    "\n",
    "flatten_transformbool, default=True\n",
    "Affects shape of transform output only when voting=’soft’ If voting=’soft’ and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).\n",
    "\n",
    "verbosebool, default=False\n",
    "If True, the time elapsed while fitting will be printed as it is completed.\n",
    "\n",
    "New in version 0.23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Voting Classifier with Hyperparameter tuning\")\n",
    "\n",
    "#VotingClassifier with hyperparameter tuning using GridSearchCV for Random Forest, SVM, and Logistic Regression:\n",
    "#hyperparameters for Random Forest (n_estimators and max_depth), SVM (C and kernel), and Logistic Regression (C and penalty). The\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "print(\"Models\")\n",
    "print(\"LogisticRegression, RandomForestClassifier, DecisionTreeClassifier, Extra Tree Classifiers\")\n",
    "print(\"Baggign Classifiers, Svc, LinearSVC\")\n",
    "print(\"XGBClassifier (XGBoost), CatBoostClassifier, LGBMClassifier (LightGBM)\")\n",
    "\n",
    "\n",
    "df_vc=df.copy()\n",
    "x_vc = df_vc[feat_list]\n",
    "y_vc = df_vc[target_y]\n",
    "x_vc_train, x_vc_test, y_vc_train, y_vc_test = train_test_split(x_vc, \n",
    "                                                                y_vc, \n",
    "                                                                test_size=test_size0, \n",
    "                                                                random_state=random_state0)\n",
    "\n",
    "print(\"Define individual classifiers with hyperparameter tuning\")\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=random_state0)\n",
    "et_classifier = ExtraTreesClassifier(random_state=random_state0)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=random_state0)\n",
    "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                                       random_state=random_state0)\n",
    "lr_classifier = LogisticRegression(max_iter=1000, \n",
    "                                   random_state=random_state0)\n",
    "svm_classifier = SVC(probability=True, \n",
    "                     random_state=random_state0)\n",
    "#linear_svc_classifier = LinearSVC(random_state=random_state0)\n",
    "xgb_classifier = XGBClassifier(random_state=random_state0)\n",
    "lgbm_classifier = LGBMClassifier(random_state=random_state0)\n",
    "\n",
    "print(\"Define hyperparameters for tuning\")\n",
    "print(\"Since we already know the parameters around which the best  indfividal odel reside, in order to save compouter pricessingtim\")\n",
    "print(\"we will make the hyperaprmaters near th eoptimimum \")\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    'et__n_estimators': [50, 100, 200],\n",
    "    'et__max_depth': [None, 10, 20],\n",
    "    'bagging__n_estimators': [50, 100, 200],\n",
    "    'bagging__base_estimator__max_depth': [None, 10, 20],\n",
    "    'lr__C': [0.1, 1, 10],\n",
    "    'lr__penalty': ['l1', 'l2'],\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__kernel': ['linear', 'rbf'],\n",
    "    'xgb__learning_rate': [0.1, 0.01],\n",
    "    'xgb__max_depth': [3, 5, 7],\n",
    "    'lgbm__learning_rate': [0.1, 0.01],\n",
    "    'lgbm__max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "\n",
    "# Define the parameter grids for each classifier\n",
    "param_grid_log_reg = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_random_forest = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
    "param_grid_decision_tree = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}\n",
    "param_grid_extra_trees = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
    "param_grid_bagging_classifier = {'n_estimators': [10, 20, 30], 'base_estimator__max_depth': [None, 10, 20]}\n",
    "param_grid_svc = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': ['scale', 'auto']}\n",
    "param_grid_xgb_classifier = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "param_grid_lgbm_classifier = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Create a Voting Classifier\")\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_classifier),\n",
    "    ('et', et_classifier),\n",
    "    ('dt', dt_classifier),\n",
    "    ('bagging', bagging_classifier),\n",
    "    ('lr', lr_classifier),\n",
    "    ('svm', svm_classifier),\n",
    "#   ('linear_svc', linear_svc_classifier),\n",
    "    ('xgb', xgb_classifier),\n",
    "    ('lgbm', lgbm_classifier)\n",
    "], voting='soft')  # 'soft' for probability voting\n",
    "\n",
    "print(\"Creatin the GridSearchCV for hyperparameter tuning\")\n",
    "grid_search = GridSearchCV(estimator=voting_classifier, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=3)\n",
    "\n",
    "print(\"Fitting the model with hyperparameter tuning\")\n",
    "grid_search.fit(x_vc_train, \n",
    "                y_vc_train)\n",
    "\n",
    "print(\"Get the best model from GridSearchCV\")\n",
    "best_voting_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"After finding the best model, make predictions on the test set with the best model\")\n",
    "y_pred = best_voting_model.predict(x_vc_test)\n",
    "\n",
    "print(\"Voting Classifier with Hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate the Voting Classifier\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Best Model Accuracy: {accuracy}')\n",
    "print(f'Best Model Parameters: {grid_search.best_params_}')\n",
    "\n",
    "print(\"Additional evaluation metrics\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vc=df.copy()\n",
    "x_vc = df_vc[feat_list]\n",
    "y_vc = df_vc[target_y]\n",
    "x_vc_train, x_vc_test, y_vc_train, y_vc_test = train_test_split(x_vc, \n",
    "                                                                y_vc, \n",
    "                                                                test_size=test_size0, \n",
    "                                                                random_state=random_state0)\n",
    "\n",
    "\n",
    "# Define the parameter grids for each classifier\n",
    "param_grid_log_reg = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_random_forest = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
    "param_grid_decision_tree = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}\n",
    "param_grid_extra_trees = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
    "param_grid_bagging_classifier = {'n_estimators': [10, 20, 30], 'base_estimator__max_depth': [None, 10, 20]}\n",
    "param_grid_svc = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': ['scale', 'auto']}\n",
    "param_grid_xgb_classifier = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "param_grid_lgbm_classifier = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "\n",
    "# Create instances of classifiers\n",
    "log_reg = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "extra_trees = ExtraTreesClassifier()\n",
    "bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier())\n",
    "svc = SVC(probability=True)\n",
    "xgb_classifier = XGBClassifier()\n",
    "lgbm_classifier = LGBMClassifier()\n",
    "\n",
    "# Create parameter grid dictionary\n",
    "param_grids = {\n",
    "    'log_reg': (log_reg, param_grid_log_reg),\n",
    "    'random_forest': (random_forest, param_grid_random_forest),\n",
    "    'decision_tree': (decision_tree, param_grid_decision_tree),\n",
    "    'extra_trees': (extra_trees, param_grid_extra_trees),\n",
    "    'bagging_classifier': (bagging_classifier, param_grid_bagging_classifier),\n",
    "    'svc': (svc, param_grid_svc),\n",
    "    'xgb_classifier': (xgb_classifier, param_grid_xgb_classifier),\n",
    "    'lgbm_classifier': (lgbm_classifier, param_grid_lgbm_classifier),\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "best_models = {}\n",
    "for clf_name, (clf, param_grid) in param_grids.items():\n",
    "    print(f\"Performing hyperparameter tuning for {clf_name}...\")\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(x_vc_train, y_vc_train)\n",
    "    best_models[clf_name] = grid_search.best_estimator_\n",
    "\n",
    "# Create a voting classifier with the best-tuned models\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('log_reg', best_models['log_reg']),\n",
    "        ('random_forest', best_models['random_forest']),\n",
    "        ('decision_tree', best_models['decision_tree']),\n",
    "        ('extra_trees', best_models['extra_trees']),\n",
    "        ('bagging_classifier', best_models['bagging_classifier']),\n",
    "        ('svc', best_models['svc']),\n",
    "        ('xgb_classifier', best_models['xgb_classifier']),\n",
    "        ('lgbm_classifier', best_models['lgbm_classifier']),\n",
    "    ],\n",
    "    voting='soft'  # 'soft' for probabilities, 'hard' for majority voting\n",
    ")\n",
    "\n",
    "# Fit the voting classifier on the training data\n",
    "voting_classifier.fit(x_vc_train, y_vc_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_proba = voting_classifier.predict_proba(x_vc_test)[:, 1]\n",
    "\n",
    "# Evaluate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_vc_test, y_pred_proba)\n",
    "print(f\"Voting Classifier ROC-AUC after hyperparameter tuning: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310af8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results\")\n",
    "print(\"Accuaracy\") \n",
    "print(\"Backtesting\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc423269",
   "metadata": {},
   "source": [
    "    \n",
    "- [Top](#table_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e861d4",
   "metadata": {},
   "source": [
    "### Conclusion <a id=\"conclusion\"></a>\n",
    "\n",
    "- Euler Maruyama numerical method is computationally efficient and can make accurate approximations for SDEs, however, it cannot reach the accuracy levels of higher orders methods for complex problems and its accuracy depends on the size of the steps. \n",
    "\n",
    "\n",
    "   - [Top](#table_contents)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23b260",
   "metadata": {},
   "source": [
    "### References <a id=\"references\"></a>\n",
    "\n",
    "- Bouzoubaa, M.; Osseiran A. **\"Exotic Options and Hybrids: A Guide to Structuring, Pricing and Trading\"**\n",
    "- Zhang, P. **\"Exotic Options:A Guide to Second Generation Options\"**\n",
    "- Das, S. **\"Structured Products Volume 1: Exotic Options; Interest Rates and Currency\"**\n",
    "- Healy, J. **\"\"Applied Quantitative Finance for Equity Derivatives\"**\n",
    "- Kat, H. **\"Structured Equity Derivatives: The Definitive Guide to Exotic Options and Structured Notes. Wiley (2001)\"**\n",
    "- Overhaus, M.; Bermudez, A. ***\"Equity Hybrid Derivatives. Wiley (2007) \"***\n",
    "- Fadugba, S., Adegboyegun, B.,**\"On the convergence of euler maruyama method and milstein scheme for the solution of stochastic differential equations\"** International Journal of Applied Mathematics and Modeling\n",
    "- CQF Program. **\"Paul Wilmott on Quantitative Finance\"** New York: John Wiley & Sons, Ltd., 2006.\n",
    "- CQF Program. ***Module 3. Lecture 4 Introduction to Numerical Methodss***\n",
    "- CQF Program. ***Module 3. Lecture 5 Exotic Options***\n",
    "- CQF Program. ***Module 3. Lecture 6 Understanding Volatility***\n",
    "- CQF Program. ***Python Lab: 5 Black Scholes Option Pricing***\n",
    "- CQF Program. ***Python Lab: 6 Monte Carlo Simulation***\n",
    "- CQF Program. ***Python Lab: 7 Finite Difference Methods***\n",
    "\n",
    "\n",
    "References\n",
    "Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. (2013). An introduction to statistical learning : with applications in R. New York :Springer\n",
    "P. Tan, M. Steinbach, and V. Kumar. (2005) Introduction to Data Mining. Addison Wesley\n",
    "\n",
    "\n",
    "- [Top](#table_contents)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2493b8",
   "metadata": {},
   "source": [
    "- [Top](#table_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50c319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
